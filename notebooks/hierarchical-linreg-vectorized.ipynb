{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with parental weights of length 4, one for each feature.\n",
    "parental_weights = np.random.normal(loc=10, scale=3, size=(4,))\n",
    "parental_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll now generate new weights based on location=parental_weights,\n",
    "# and scale=1\n",
    "\n",
    "child_weights = np.random.normal(loc=parental_weights, scale=1, size=(2, 4))\n",
    "child_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the true weights of the system.\n",
    "\n",
    "We are now going to attempt to learn them in a Bayesian fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samps = 100\n",
    "n_tasks = 2\n",
    "n_weights = 4\n",
    "data = np.random.normal(loc=3, scale=4, size=(n_tasks, n_samps, n_weights))\n",
    "\n",
    "# Now, on the last 20 samples on the 2nd task, set everything to zeros\n",
    "# to indicate that it has nothing in there.\n",
    "data[80:, 1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape, child_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.einsum('ijk, ik -> ij', data, child_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to write a hierarchical linear regression model that handles this particular case of imbalanced number of samples.\n",
    "\n",
    "If we are able to recover back the original weights, then zero-padding could be a very powerful technique to deal with multiple learning tasks that also have non-equal numbers of samples that also have non-overlapping sample indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.batched_dot(data, child_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as hierarchical_linear_model:\n",
    "    w_parent = pm.Normal(\"w_parent\", mu=0, sd=1, shape=(4,))\n",
    "    \n",
    "    # Broadcasting will give us 4 child weights drawn from w_parent, \n",
    "    # I think.\n",
    "    w_child = pm.Normal(\"w_child\", mu=w_parent, sd=1, shape=(2, 4))\n",
    "    \n",
    "    sd = pm.HalfCauchy(\"sd\", beta=10)\n",
    "    \n",
    "    # mu = pm.Deterministic(\"mu\", np.einsum('ijk, kj -> ij', data, w_child))\n",
    "    mu = pm.Deterministic(\"mu\", tt.batched_dot(data, w_child))\n",
    "    like = pm.Normal(\"like\", mu=mu, sd=sd, observed=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with hierarchical_linear_model:\n",
    "    # trace = pm.sample(2000, cores=1)\n",
    "    approx = pm.fit(100000)\n",
    "    trace = approx.sample(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace[\"w_child\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace[\"w_parent\"].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parental_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're close!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace[\"w_child\"].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace[\"w_child\"].std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! I think that this works, just that something is not right with NUTS because of gradient issues (we get zeros on diagonal of mass matrix). I'm going to show this experiment to the PyMC devs to see what I might be doing wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n",
    "\n",
    "Just to recap what we've done here.\n",
    "\n",
    "We have two learning tasks that involve the same _kind_ of input data, but don't have exactly aligned samples. In the first learning task, we have 100 iid samples; in the 2nd learning task we have 80 iid samples. In our data matrix, the 2nd task's 80 iid samples are not necessarily aligned with the 100 iid samples from the 1st task. One other assumption we have baked into this model is that the weights, while given a set for each task, are shared from a parental prior, hence there is parameter sharing amongst the learning tasks, though not in our usual \"classical\" sense.\n",
    "\n",
    "By appending zero-padding, we should be able to generalize this to multi-task neural network learning with non-overlapping samples. [Thomas Wiecki](https://twiecki.io/blog/2018/08/13/hierarchical_bayesian_neural_network/) has a great blog post on how to do it, though he didn't deal with the \"number of samples\" issue, which I tried to add here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian",
   "language": "python",
   "name": "bayesian"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
