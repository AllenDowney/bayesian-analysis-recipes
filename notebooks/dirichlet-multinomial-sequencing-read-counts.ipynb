{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Statistical Model for Picking Most Abundant Sequences\n",
    "\n",
    "by Eric J. Ma and Arkadij Kummer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In DNA sequencing, when we look at the raw sequencing data, we might see that a sample contains a multiple of sequences, each with different read counts. If we are to use this data in machine learning, one modelling choice we usually need make is to pick one of those sequences as the representative one, or else omit the data from our downstream model if we deem it to be of insufficient quality.\n",
    "\n",
    "By insufficient quality, we usually refer to one of the following scenarios:\n",
    "\n",
    "1. Low read counts.\n",
    "1. Mixture of sequences with very similar read counts.\n",
    "1. Both of the above.\n",
    "\n",
    "A common practice in the DNA sequencing world is to pick a threshold (e.g. discard everything with less than 10 read counts). We wondered whether there might be a statistically-principled way for us to identify for which samples we could \"just pick the highest value\", and for which samples we need to omit from downstream ML purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example DNA Sequencing Data\n",
    "\n",
    "As an example, we may have sequencing data that look like the following:\n",
    "\n",
    "| Sequence | Count |\n",
    "|----------|-------|\n",
    "| AGGAT... |  17   |\n",
    "| AGGTT... |  3    |\n",
    "| ACGAT... |  121  |\n",
    "\n",
    "This data can be cast in an [\"urn problem\"](https://en.m.wikipedia.org/wiki/Urn_problem). From a generative model standpoint, we consider sequences to be present at a range of proportions in an urn, and the DNA sequencer samples them, giving rise to the read count data that we observe. The number of categories (i.e. unique sequences) varies per urn (i.e. a DNA sample sent for sequencing). An appropriate statistical model here for each DNA sample is the Dirichlet-Multinomial model, which has a number of nice properties that we can take advantage of:\n",
    "\n",
    "1. The multinomial distribution story gives sample counts from groups of data.\n",
    "1. The Dirichlet distribution is the conjugate prior for the multinomial distribution probability parameter $p$, meaning that simple addition arithmetic can be used to compute the Dirichlet posterior.\n",
    "1. Dirichlet distribution marginals for any of the categories is Beta-distributed, which is also analytically computable.\n",
    "\n",
    "Before we go on to the problem, let's take a very quick and cursory look that powers this, just as a reminder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirichlet-Multinomial Bayesian Updating\n",
    "\n",
    "Very briefly, if we have the following prior distribution\n",
    "\n",
    "$$p_prior \\sim Dir([\\alpha_1, \\alpha_2, ..., \\alpha_n])$$\n",
    "\n",
    "which models the probability of observing each of $n$ groups, and we observe actual counts for the $n$ groups\n",
    "\n",
    "$$c = [c_1, c_2, ..., c_n]$$\n",
    "\n",
    "Then the posterior distribution of the $\\alpha$s is \n",
    "\n",
    "$$p_{posterior} \\sim Dir([\\alpha_1 + c_1, \\alpha_2 + c_2, ..., \\alpha_n + c_n])$$\n",
    "\n",
    "Moreover, the marginal distribution of each of the probabilities $1, 2, ... n$ is given by a Beta distribution\n",
    "\n",
    "$$p_{n} \\sim Beta(\\alpha_n + c_n, \\sum_{iâ‰ n}(\\alpha_i + c_i))$$\n",
    "\n",
    "Taking advantage of conjugacy means we can avoid doing MCMC, and simply turn to arithmetic instead.\n",
    "\n",
    "Here are a few examples of posterior updating with a prior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import dirichlet, multinomial, beta\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_posterior(read_counts):\n",
    "    xs = np.linspace(0, 1, 1000)\n",
    "\n",
    "    prior_counts = np.ones(len(read_counts))\n",
    "    posterior_counts = prior_counts + read_counts\n",
    "\n",
    "    for count in posterior_counts:\n",
    "        marginal = beta(count, posterior_counts.sum() - count).pdf(xs)\n",
    "        plt.plot(xs, marginal, label=int(count))\n",
    "    plt.xlim(-0.05, 1.05)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_posterior([127, 1, 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_posterior([2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_posterior([200, 300, 400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How might we devise a rule for whether or not to pick the top sequence?\n",
    "\n",
    "We could look at a particular percentage interval of the posterior distribution of the top count, and ask if it overlaps with the rest of the posterior distributions on the same percentage interval.\n",
    "\n",
    "For example, let's see if the 99% interval of the $[2, 3]$ scenario overlaps (if it's not obvious from above, it should)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kth_largest(a, k):\n",
    "    return np.sort(a)[-k]\n",
    "\n",
    "def keep_largest_test(read_counts):\n",
    "\n",
    "    prior_counts = np.ones(len(read_counts))\n",
    "    posterior_counts = prior_counts + read_counts\n",
    "\n",
    "    max1 = kth_largest(posterior_counts, 1)\n",
    "    max2 = kth_largest(posterior_counts, 2)\n",
    "    \n",
    "    max1dist = beta(max1, sum(posterior_counts) - max1)\n",
    "    max2dist = beta(max2, sum(posterior_counts) - max2)\n",
    "    quantiles = [0.025, 0.975]\n",
    "    \n",
    "    max1l, max1u = max1dist.ppf(quantiles)\n",
    "    max2l, max2u = max2dist.ppf(quantiles)\n",
    "    \n",
    "    if max1l > max2u:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_largest_test([200, 300, 400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian",
   "language": "python",
   "name": "bayesian"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
