{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, I want to take a deep dive into sampling from a log likelihood with vanilla MCMC. This will help me build my intuition around Bayesian statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap Bayes' Rule\n",
    "\n",
    "Bayes' rule is as follows:\n",
    "\n",
    "$$P(H|D)= \\frac{P(D|H)P(H)}{P(D)}$$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $P(H)$ is the prior distribution on $H$ having not seen the data.\n",
    "- $P(H|D)$ is the posterior belief on $H$ having seen the data $D$.\n",
    "- $P(D|H)$ is the likelihood of the data.\n",
    "- $P(D)$ is the normalizing constant, or the probability of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating $P(X)$s into code\n",
    "\n",
    "I find I don't _truly_ understand something until I am able to translate it into code. Here, I will attempt to do so.\n",
    "\n",
    "I will start with the components that I know. \n",
    "\n",
    "Firstly, I know how to calculate the likelihood of data given a hypothesis. This involves writing a function that takes in data points, and calculates the sum of log likelihoods (or product of likelihoods) assuming that the samples of data are i.i.d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_loglike(x, **norm_params):\n",
    "    dist = norm(**norm_params)\n",
    "    \n",
    "    return np.sum(dist.logpdf(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use some fake data to test our understanding.\n",
    "\n",
    "I will generate fake data from a $N(3,1)$ distribution, but calculate the log likelihood under a $N(0,1)$ distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = norm(loc=3, scale=1).rvs(100)\n",
    "\n",
    "norm_loglike(xs, loc=0, scale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compare that with the log likelihood from the actual distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_loglike(xs, loc=3, scale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we still haven't had any \"priors\" injected here. Let's try it out with priors placed on the location of the Normal distribution, $m$.\n",
    "\n",
    "Assume $m \\sim N(0,10)$. In probability notation, \n",
    "\n",
    "$$P(m) = \\frac{1}{\\sigma_m \\sqrt{2 \\pi}} e^\\frac{-(m-\\mu_m)}{2 \\sigma_m^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglike_prior(m):\n",
    "    dist = norm(loc=0, scale=10)\n",
    "    return np.sum(dist.logpdf(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test a few prior values, and calculate their log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loglike_prior(0))\n",
    "print(loglike_prior(-1))\n",
    "print(loglike_prior(1))\n",
    "print(loglike_prior(-100))\n",
    "print(loglike_prior(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically what we would expect. $0$ has the highest log likelihood, and log-likelihood goes down symmetrically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, however, we need to use this prior in conjunction with the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_log_prob(m, xs):\n",
    "    return loglike_prior(m) + norm_loglike(xs, loc=m, scale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we multiply the prior probability distribution by the likelihood probability distribution, we are doing nothing more than summing up their log likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lls = []\n",
    "for m in np.linspace(-10, 10, 100):\n",
    "    ll = joint_log_prob(m, xs)\n",
    "    lls.append(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(-10, 10, 100), lls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(-10, 10, 100)[lls.index(max(lls))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo, this is close to the true value!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Version\n",
    "\n",
    "Let's now do it by _sampling_.\n",
    "\n",
    "We are going to use the [Metropolis-Hastings algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm).\n",
    "\n",
    "Briefly, it works as follows:\n",
    "\n",
    "- Initialize an arbitrary point.\n",
    "- Choose density to propose new point (we will use $N(m_{t-1}, 1)$).\n",
    "- For each iteration:\n",
    "    - Generate candidate new candidate $m_t$.\n",
    "    - Calculate acceptance ratio. We take advantage of the joint log probability L, by passing in the proposed value of $m_t$ into the logprob function, i.e. $L(m_t)$ and comparing it to $L(m_{t-1})$.\n",
    "    - Now, we compute the ratio $r = \\frac{L(m_t)}{L(m_{t-1})}$. \n",
    "    - Generate a new random number on the interval $p \\sim U(0, 1)$.\n",
    "    - Compare $p$ to $r$. \n",
    "        - If $p \\leq r$, accept $m_t$.\n",
    "        - If $p \\gt r$, reject $m_t$ and continue sampling again with $m_{t-1}$.\n",
    "        \n",
    "Let's write it in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metropolis-Hastings Sampling\n",
    "m_prev = np.random.normal(0, 1)\n",
    "\n",
    "\n",
    "history = dict()\n",
    "ratio_history = dict()\n",
    "for i in range(1000):\n",
    "    history[i] = m_prev\n",
    "    m_t = np.random.normal(m_prev, 5)\n",
    "    \n",
    "    # Compute negative joint log likelihood\n",
    "    L_t = np.exp(joint_log_prob(m_t, xs))\n",
    "    L_prev = np.exp(joint_log_prob(m_prev, xs))\n",
    "    \n",
    "    ratio = L_t / L_prev\n",
    "    ratio_history[i] = L_t\n",
    "    p = np.random.uniform(0, 1)\n",
    "    if p <= ratio:\n",
    "        m_prev = m_t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([i for i in history.values()]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked!\n",
    "\n",
    "Now lies a challenge: How do we design an API that can handle not just `m` as the only variable, but arbitrary numbers of variables that have to be learned?\n",
    "\n",
    "Let's try this out with a model that has two random variables instead of one. Here, we'll add variance of the likelihood to the list of RVs.\n",
    "\n",
    "Here:\n",
    "\n",
    "$$\\mu \\sim N(0, 10)$$\n",
    "$$\\sigma \\sim Exp(2)$$\n",
    "$$Y \\sim N(\\mu, \\sigma)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon\n",
    "\n",
    "def normal_prior_loglike(mu):\n",
    "    return np.sum(norm(0, 10).logpdf(mu))\n",
    "\n",
    "def sigma_prior_loglike(sigma):\n",
    "    return np.sum(expon(2).logpdf(sigma))\n",
    "\n",
    "def data_loglike(mu, sigma, xs):\n",
    "    return np.sum(norm(mu, sigma).logpdf(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loglike(mu, sigma, xs):\n",
    "    return normal_prior_loglike(mu) + sigma_prior_loglike(sigma) + data_loglike(mu, sigma, xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metropolis-Hastings Sampling\n",
    "mu_prev = np.random.normal(0, 1)\n",
    "sigma_prev = np.random.normal(0, 1)\n",
    "\n",
    "mu_history = dict()\n",
    "sigma_history = dict()\n",
    "ratio_history = dict()\n",
    "\n",
    "# Let's test this with 100 steps.\n",
    "for i in range(100):\n",
    "    mu_history[i] = mu_prev\n",
    "    sigma_history[i] = sigma_prev\n",
    "    mu_t = np.random.normal(mu_prev, 5)\n",
    "    sigma_t = np.random.normal(sigma_prev, 5)\n",
    "    \n",
    "    # Compute negative joint log likelihood\n",
    "    LL_t = model_loglike(mu_t, sigma_t, xs)\n",
    "    LL_prev = model_loglike(mu_prev, sigma_prev, xs)\n",
    "    \n",
    "    ratio = np.exp(LL_t - LL_prev)\n",
    "    ratio_history[i] = ratio\n",
    "    p = np.random.uniform(0, 1)\n",
    "    if p <= ratio:\n",
    "        m_prev = m_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no, we get NaNs and infs in there! Why? Because we passed in invalid values into the logpdf of sigma. \n",
    "\n",
    "Sigma is bounded as a positive distribution, so we need to to somehow either:\n",
    "\n",
    "- Restrict sigma's proposal distribution to be positive, or\n",
    "- Transform sigma such that it lives on the real line, propose a new value on the real line, and then transform it back to original sigma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to decompose this into its constituent components.\n",
    "\n",
    "1. We have a `sampler` to sample from the joint log-likelihood, which should accept proposed random variable values and data, and return a scalar.\n",
    "2. We have a `model` specification, which should accept data and return a log-likelihood function and the random variables values to evaluate.\n",
    "\n",
    "An API sketch might look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian",
   "language": "python",
   "name": "bayesian"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
