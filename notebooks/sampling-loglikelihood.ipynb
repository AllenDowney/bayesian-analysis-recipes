{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Probability and Computational Bayesian Statistics\n",
    "\n",
    "In Bayesian statistics,\n",
    "we often say that we are \"sampling\" from a posterior distribution\n",
    "to estimate what parameters could be,\n",
    "given a model structure and data.\n",
    "What exactly is happening here?\n",
    "\n",
    "Examples that I have seen on \"how sampling happens\"\n",
    "tends to focus on an overly-simple example\n",
    "of sampling from a single distribution with known parameters.\n",
    "I was wondering if I could challenge myself\n",
    "to come up with a \"simplest complex example\"\n",
    "that would illuminate ideas that were obscure to me before.\n",
    "In this essay, I would like to share that knowledge with you,\n",
    "and hopefully build up your intuition behind\n",
    "what is happening in computational Bayesian inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Distributions\n",
    "\n",
    "We do need to have a working understanding\n",
    "of what a probability distribution is before we can go on.\n",
    "Without going down deep technical and philosophical rabbit holes\n",
    "(I hear they are deep),\n",
    "I'll start by proposing\n",
    "that \"a probability distribution is a Python object\n",
    "that has a math function\n",
    "that allocates credibility points onto the number line\".\n",
    "\n",
    "Because we'll be using the normal distribution extensively in this essay,\n",
    "we'll start off by examining that definition\n",
    "in the context of the standard normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Object Implementation\n",
    "\n",
    "Since the normal distribution is an object,\n",
    "I'm implying here that it can hold state.\n",
    "What might that state be?\n",
    "Well, we know from math that probability distributions have parameters,\n",
    "and that the normal distribution\n",
    "has the \"mean\" and \"variance\" parameters defined.\n",
    "In Python code, we might write it as:\n",
    "\n",
    "```python\n",
    "class Normal:\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Density Function\n",
    "\n",
    "Now, I also stated that the normal distribution has a math function\n",
    "that we can use to allocate credibility points to the number line.\n",
    "This function also has a name,\n",
    "called a \"probability distribution function\", or the \"PDF\".\n",
    "Using this, we may then extend extend this object\n",
    "with a method called `.pdf(x)`,\n",
    "that returns a number\n",
    "giving the number of credibility points\n",
    "assigned to the value of `x` passed in.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class Normal:\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def pdf(self, x):\n",
    "        return (\n",
    "            1 / np.sqrt(2 * self.sigma ** 2 * np.pi)\n",
    "            * np.exp(\n",
    "                - (x - self.mu) ** 2\n",
    "                / 2 * self.sigma ** 2\n",
    "            )\n",
    "```\n",
    "\n",
    "If we pass in a number `x` from the number line,\n",
    "we will get back another number that tells us\n",
    "the number of credibility points given to that value `x`,\n",
    "under the state of the normal distribution instantiated.\n",
    "We'll call this $P(x)$.\n",
    "\n",
    "To simplify the implementation used here,\n",
    "we are going to borrow some machinery already available to us\n",
    "in the Python scientific computing ecosystem,\n",
    "particularly from the SciPy stats module,\n",
    "which gives us reference implementations of probability distributions.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class Normal:\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "        # We instantiate the distribution object here.\n",
    "        self.dist = norm(loc=mu, scale=sigma)\n",
    "\n",
    "    def pdf(self, x):\n",
    "        # Now, our PDF class method is simplified to be just a wrapper.\n",
    "        return self.dist.pdf(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Probability\n",
    "\n",
    "A common task in Bayesian inference is computing the likelihood of data.\n",
    "Let's assume that the data ${X_1, X_2, ... X_i}$ generated\n",
    "are independent and identically distributed,\n",
    "(the famous _i.i.d._ term comes from this).\n",
    "This means, then, that the joint probability of the data that was generated\n",
    "is equivalent to the product of the individual probabilities of each datum:\n",
    "\n",
    "$$P(X_1, X_2, ... X_i) = P(X_1) P(X_2) ... P(X_i)$$\n",
    "\n",
    "(We have to know the rules of probability to know this result;\n",
    "it is a topic for a different essay.)\n",
    "\n",
    "If you remember the notation above,\n",
    "each $P(X_i)$ is an evaluation of $X_i$\n",
    "on the distribution's probability density function.\n",
    "It being a probability value means it is bound between 0 and 1.\n",
    "However, multiplying many probabilities together\n",
    "usually will result in issues with underflow computationally,\n",
    "so in evaluating likelihoods,\n",
    "we usually stick with log-likelihoods instead.\n",
    "By the usual rules of math, then:\n",
    "\n",
    "$$\\log P(X_1, X_2, ..., X_i) = \\sum_{j=1}^{i}\\log P(X_i)$$\n",
    "\n",
    "To our normal distribution class,\n",
    "we can now add in another class method\n",
    "that computes the sum of log likelihoods\n",
    "evaluated at a bunch of i.i.d. data points.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class Normal:\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "        # We instantiate the distribution object here.\n",
    "        self.dist = norm(loc=mu, scale=sigma)\n",
    "\n",
    "    def pdf(self, x):\n",
    "        # Now, our PDF class method is simplified to be just a wrapper.\n",
    "        return self.dist.pdf(x)\n",
    "\n",
    "    def logpdf(self, x):\n",
    "        return self.dist.logpdf(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n",
    "\n",
    "Informally, a \"random variable\" is nothing more than\n",
    "a variable whose quantity is non-deterministic (hence random)\n",
    "but whose probability of taking on a certain value\n",
    "can be described by a probability distribution.\n",
    "\n",
    "According to the Wikipedia definition of a [random variable][rv]:\n",
    "\n",
    "> A random variable has a probability distribution, which specifies the probability of its values.\n",
    "\n",
    "[rv]: https://en.wikipedia.org/wiki/Random_variable\n",
    "\n",
    "As such, it may be tempting to conceive of a random variable\n",
    "as an object that has a probability distribution attribute attached to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizations of a Random Variable\n",
    "\n",
    "On the other hand, it can also be convenient to invert that relationship,\n",
    "and claim that a probability distribution\n",
    "can generate realizations of a random variable.\n",
    "The latter is exactly how SciPy distributions are implemented:\n",
    "\n",
    "```python\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Normal distribution can generate realizations of an RV\n",
    "# The following returns a NumPy array of 10 draws\n",
    "# from a standard normal distribution.\n",
    "norm(loc=0, scale=1).rvs(10)\n",
    "```\n",
    "\n",
    "> A \"realization\" of a random variable is nothing more than\n",
    "> generating a random number\n",
    "> whose probability of being generated\n",
    "> is defined by the random variable's probability density function.\n",
    "\n",
    "Because the generation of realizations of a random variable\n",
    "is equivalent to sampling from a probability distribution,\n",
    "we can extend our probability distribution definition\n",
    "to include a `.sample(n)` method:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class Normal:\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "        # We instantiate the distribution object here.\n",
    "        self.dist = norm(loc=mu, scale=sigma)\n",
    "\n",
    "    # ...\n",
    "\n",
    "    def sample(self, n):\n",
    "        return self.dist.rvs(n)\n",
    "```\n",
    "\n",
    "Now, if we draw 10 realizations of a normally distributed random variable,\n",
    "and the drawing of each realization has no dependence of any kind\n",
    "on the previous draw,\n",
    "then we can claim that each draw is **independent**\n",
    "and **identically distributed**.\n",
    "This is where the fabled \"_iid_\" term in undergraduate statistics classes\n",
    "comes from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating $P(X)$s into code\n",
    "\n",
    "I find I don't _truly_ understand something until I am able to translate it into code. Here, I will attempt to do so.\n",
    "\n",
    "I will start with the components that I know. \n",
    "\n",
    "Firstly, I know how to calculate the likelihood of data given a hypothesis. This involves writing a function that takes in data points, and calculates the sum of log likelihoods (or product of likelihoods) assuming that the samples of data are i.i.d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_loglike(x, **norm_params):\n",
    "    dist = norm(**norm_params)\n",
    "    \n",
    "    return np.sum(dist.logpdf(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use some fake data to test our understanding.\n",
    "\n",
    "I will generate fake data from a $N(3,1)$ distribution, but calculate the log likelihood under a $N(0,1)$ distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = norm(loc=3, scale=1).rvs(100)\n",
    "\n",
    "norm_loglike(xs, loc=0, scale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compare that with the log likelihood from the actual distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_loglike(xs, loc=3, scale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we still haven't had any \"priors\" injected here. Let's try it out with priors placed on the location of the Normal distribution, $m$.\n",
    "\n",
    "Assume $m \\sim N(0,10)$. In probability notation, \n",
    "\n",
    "$$P(m) = \\frac{1}{\\sigma_m \\sqrt{2 \\pi}} e^\\frac{-(m-\\mu_m)}{2 \\sigma_m^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglike_prior(m):\n",
    "    dist = norm(loc=0, scale=10)\n",
    "    return np.sum(dist.logpdf(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test a few prior values, and calculate their log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loglike_prior(0))\n",
    "print(loglike_prior(-1))\n",
    "print(loglike_prior(1))\n",
    "print(loglike_prior(-100))\n",
    "print(loglike_prior(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically what we would expect. $0$ has the highest log likelihood, and log-likelihood goes down symmetrically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, however, we need to use this prior in conjunction with the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_log_prob(m, xs):\n",
    "    return loglike_prior(m) + norm_loglike(xs, loc=m, scale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we multiply the prior probability distribution by the likelihood probability distribution, we are doing nothing more than summing up their log likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lls = []\n",
    "for m in np.linspace(-10, 10, 100):\n",
    "    ll = joint_log_prob(m, xs)\n",
    "    lls.append(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(-10, 10, 100), lls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(-10, 10, 100)[lls.index(max(lls))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo, this is close to the true value!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Version\n",
    "\n",
    "Let's now do it by _sampling_.\n",
    "\n",
    "We are going to use the [Metropolis-Hastings algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm).\n",
    "\n",
    "Briefly, it works as follows:\n",
    "\n",
    "- Initialize an arbitrary point.\n",
    "- Choose density to propose new point (we will use $N(m_{t-1}, 1)$).\n",
    "- For each iteration:\n",
    "    - Generate candidate new candidate $m_t$.\n",
    "    - Calculate acceptance ratio. We take advantage of the joint log probability L, by passing in the proposed value of $m_t$ into the logprob function, i.e. $L(m_t)$ and comparing it to $L(m_{t-1})$.\n",
    "    - Now, we compute the ratio $r = \\frac{L(m_t)}{L(m_{t-1})}$. \n",
    "    - Generate a new random number on the interval $p \\sim U(0, 1)$.\n",
    "    - Compare $p$ to $r$. \n",
    "        - If $p \\leq r$, accept $m_t$.\n",
    "        - If $p \\gt r$, reject $m_t$ and continue sampling again with $m_{t-1}$.\n",
    "        \n",
    "Let's write it in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metropolis-Hastings Sampling\n",
    "m_prev = np.random.normal(0, 1)\n",
    "\n",
    "\n",
    "history = dict()\n",
    "ratio_history = dict()\n",
    "for i in range(1000):\n",
    "    history[i] = m_prev\n",
    "    m_t = np.random.normal(m_prev, 5)\n",
    "    \n",
    "    # Compute negative joint log likelihood\n",
    "    L_t = np.exp(joint_log_prob(m_t, xs))\n",
    "    L_prev = np.exp(joint_log_prob(m_prev, xs))\n",
    "    \n",
    "    ratio = L_t / L_prev\n",
    "    ratio_history[i] = L_t\n",
    "    p = np.random.uniform(0, 1)\n",
    "    if p <= ratio:\n",
    "        m_prev = m_t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([i for i in history.values()]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked!\n",
    "\n",
    "Now lies a challenge: How do we design an API that can handle not just `m` as the only variable, but arbitrary numbers of variables that have to be learned?\n",
    "\n",
    "Let's try this out with a model that has two random variables instead of one. Here, we'll add variance of the likelihood to the list of RVs.\n",
    "\n",
    "Here:\n",
    "\n",
    "$$\\mu \\sim N(0, 10)$$\n",
    "$$\\sigma \\sim Exp(2)$$\n",
    "$$Y \\sim N(\\mu, \\sigma)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon\n",
    "\n",
    "def normal_prior_loglike(mu):\n",
    "    return np.sum(norm(0, 10).logpdf(mu))\n",
    "\n",
    "def sigma_prior_loglike(sigma):\n",
    "    return np.sum(expon(scale=2).logpdf(sigma))\n",
    "\n",
    "def data_loglike(mu, sigma, xs):\n",
    "    return np.sum(norm(mu, sigma).logpdf(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loglike(mu, sigma, xs):\n",
    "    return normal_prior_loglike(mu) + sigma_prior_loglike(sigma) + data_loglike(mu, sigma, xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metropolis-Hastings Sampling\n",
    "mu_prev = np.random.normal(0, 1)\n",
    "sigma_prev = np.random.normal(0, 1)\n",
    "\n",
    "mu_history = dict()\n",
    "sigma_history = dict()\n",
    "ratio_history = dict()\n",
    "\n",
    "for i in range(1000):\n",
    "    mu_history[i] = mu_prev\n",
    "    sigma_history[i] = sigma_prev\n",
    "    mu_t = np.random.normal(mu_prev, 5)\n",
    "    sigma_t = np.random.normal(sigma_prev, 5)\n",
    "    \n",
    "    # Compute negative joint log likelihood\n",
    "    LL_t = model_loglike(mu_t, sigma_t, xs)\n",
    "    LL_prev = model_loglike(mu_prev, sigma_prev, xs)\n",
    "    \n",
    "    ratio = np.exp(LL_t - LL_prev)\n",
    "    ratio_history[i] = ratio\n",
    "    p = np.random.uniform(0, 1)\n",
    "    if p <= ratio:\n",
    "        m_prev = m_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no, we get NaNs and infs in there! Why? Because we passed in invalid values into the logpdf of sigma. \n",
    "\n",
    "Sigma is bounded as a positive distribution, so we need to to somehow either:\n",
    "\n",
    "1. Restrict sigma's proposal distribution to be positive, or\n",
    "1. Transform sigma such that it lives on the real line, propose a new value on the real line, and then transform it back to original sigma.\n",
    "\n",
    "Let's try the first strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metropolis-Hastings Sampling\n",
    "mu_prev = np.random.normal(0, 1)\n",
    "sigma_prev = np.random.exponential(scale=1)\n",
    "\n",
    "mu_history = dict()\n",
    "sigma_history = dict()\n",
    "ratio_history = dict()\n",
    "\n",
    "for i in range(1000):\n",
    "    mu_history[i] = mu_prev\n",
    "    sigma_history[i] = sigma_prev\n",
    "    mu_t = np.random.normal(mu_prev, 0.1)\n",
    "    sigma_t = abs(np.random.normal(sigma_prev, 0.1))\n",
    "    \n",
    "    # Compute negative joint log likelihood\n",
    "    LL_t = model_loglike(mu_t, sigma_t, xs)\n",
    "    LL_prev = model_loglike(mu_prev, sigma_prev, xs)\n",
    "    \n",
    "    \n",
    "    # Calculate the ratio from the difference in log-likelihoods\n",
    "    # (or a.k.a. ratio of likelihoods)\n",
    "    diff_log_like = LL_t - LL_prev\n",
    "    if diff_log_like > 0:\n",
    "        ratio = 1\n",
    "    else:\n",
    "        ratio = np.exp(diff_log_like)\n",
    "    if np.isinf(ratio) or np.isnan(ratio):\n",
    "        raise ValueError(f\"LL_t: {LL_t}, LL_prev: {LL_prev}\")\n",
    "                \n",
    "    ratio_history[i] = ratio\n",
    "    p = np.random.uniform(0, 1)\n",
    "    \n",
    "    if ratio >= p:\n",
    "        mu_prev = mu_t\n",
    "        sigma_prev = sigma_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sigma_history.values(), label=\"sigma\")\n",
    "plt.hist(mu_history.values(), label=\"mu\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(sigma_history.values())).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(mu_history.values())).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo! It works!\n",
    "\n",
    "Now, let's try the second option, where we propose a number in real space, then transform it into the space of the distribution support, and then evaluate the new proposed sample.\n",
    "\n",
    "Doing so lets us use \"standard\" code to propose new distributions.\n",
    "\n",
    "With a positive distribution ($X > 0$), a logical transform for the distribution is the log-transform, as it is simple, invertible, and won't have invalid values ($ln(0)$ is impossible here).\n",
    "\n",
    "Because of this, the inverse of the transformation is going to be an exponential. Hence, we will sample in unconstrained, transformed space, use the inverse transform to go back to support space, and evaluate log likelihoods in support space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metropolis-Hastings Sampling\n",
    "mu_prev = np.random.normal(0, 1)\n",
    "sigma_prev = np.random.normal(0, 1)\n",
    "\n",
    "mu_history = dict()\n",
    "sigma_history = dict()\n",
    "ratio_history = dict()\n",
    "\n",
    "for i in range(1000):\n",
    "    mu_t = np.random.normal(mu_prev, 0.1)\n",
    "    # We'll make the changes here.\n",
    "    # Firstly, we sample from the normal distribution.\n",
    "    sigma_t = np.random.normal(sigma_prev, 0.1)\n",
    "    # Then, we transform the distribution by its inverse transformation.\n",
    "    # We will have to find a way to encapsulate this routine with a class method or function.\n",
    "    sigma_t_tfm = np.exp(sigma_t)\n",
    "    sigma_prev_tfm = np.exp(sigma_prev)\n",
    "\n",
    "    # Record history\n",
    "    mu_history[i] = mu_prev\n",
    "    sigma_history[i] = sigma_prev_tfm\n",
    "\n",
    "    # Compute negative joint log likelihood\n",
    "    LL_t = model_loglike(mu_t, sigma_t_tfm, xs)\n",
    "    LL_prev = model_loglike(mu_prev, sigma_prev_tfm, xs)\n",
    "\n",
    "    # Calculate the ratio from the difference in log-likelihoods\n",
    "    # (or a.k.a. ratio of likelihoods)\n",
    "    diff_log_like = LL_t - LL_prev\n",
    "    if diff_log_like > 0:\n",
    "        ratio = 1\n",
    "    else:\n",
    "        ratio = np.exp(diff_log_like)\n",
    "    if np.isinf(ratio) or np.isnan(ratio):\n",
    "        raise ValueError(f\"LL_t: {LL_t}, LL_prev: {LL_prev}\")\n",
    "\n",
    "    ratio_history[i] = ratio\n",
    "    p = np.random.uniform(0, 1)\n",
    "\n",
    "    if ratio >= p:\n",
    "        mu_prev = mu_t\n",
    "        sigma_prev = sigma_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(list(sigma_history.values()), label=\"sigma\")\n",
    "plt.hist(list(mu_history.values()), label=\"mu\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the long tail of invalid values - that comes from the MCMC sampler stepping on its way to the region where the log likelihood is largest, but yet still isn't there. If we plot the histogram slightly differently, we will see what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(sigma_history)), list(sigma_history.values()), label=\"sigma\")\n",
    "plt.plot(range(len(mu_history)), list(mu_history.values()), label=\"mu\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, there's a bit of a \"burn-in\" that the MCMC sampler needs before it reaches the region of \"optimal\" sampling. From manually observing the MCMC trace, we can see that sampling stabilizes around the correct values after 200+ steps. Let's set 200 to be the boundary at which we start plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(800), list(sigma_history.values())[200:], label=\"sigma\")\n",
    "plt.plot(range(800), list(mu_history.values())[200:], label=\"mu\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(list(sigma_history.values())[200:], label=\"sigma\")\n",
    "plt.hist(list(mu_history.values())[200:], label=\"mu\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks much better now! Notice how we've also recovered back the correct values, with associated uncertainty modelled in as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing an API for Sampling\n",
    "\n",
    "Now, the code for Metropolis-Hastings sampling has been copied many times over at this point, so it's about time to design an API that can handle arbitrary numbers of samples and arbitrary numbers of scalar random variables.\n",
    "\n",
    "Here are the design notes that we need to take care of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized API\n",
    "\n",
    "Firstly, the sampler API has to be consistent. A starter design might be to accept a model+data log-likelihood and its associated parameters, though thinking further about it, one might want to instead accept a \"container\" of sorts that wraps everything together (i.e. the data + model parameters + joint log likelihood)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Considerations\n",
    "\n",
    "Distributions need to have automatic transformations enabled.\n",
    "We need to be able to pass in a value in unconstrained, transformed space, but evaluate the log-likelihood in constrained, support space (wherever applicable). \n",
    "\n",
    "In PyMC3, we have context managers that handle things, but maybe a simpler way of handling this is to wrap distributions in a Python object, and provide the appropriate class methods, perhaps as follows:\n",
    "\n",
    "```python\n",
    "class Exponential(PositiveDistribution):\n",
    "    def sumlogpdf(x):\n",
    "        return np.sum(self.dist.logpdf(x))\n",
    "    \n",
    "    def sumlogpdf_tfm(x):\n",
    "        x = self.inverse_transform(x)\n",
    "        return self.logpdf(x)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In doing so, we could possibly avoid needing to explicitly specify proposal distributions and their transformations in the sampling loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distribution(object):\n",
    "    def __init__(self):\n",
    "        return NotImplementedError(\"Do not use abstract distribution class!\")\n",
    "\n",
    "    def transform(self, x):\n",
    "        \"\"\"Invertible transform that converts real number to distribution support.\"\"\"\n",
    "        return x\n",
    "    \n",
    "    def inv_transform(self, x):\n",
    "        \"\"\"Inverse of transform.\"\"\"\n",
    "        return x\n",
    "    \n",
    "    def sumlogpdf_tfm(self, x):\n",
    "        x = self.inv_transform(x)\n",
    "        return self.sumlogpdf(x)\n",
    "    \n",
    "    def sumlogpdf(self, x):\n",
    "        return np.sum(self.dist.logpdf(x))\n",
    "    \n",
    "    def sample(self, n):\n",
    "        return self.dist.rvs(n)\n",
    "\n",
    "    \n",
    "class PositiveDistribution(Distribution):\n",
    "    def transform(self, x):\n",
    "        \"\"\"Transformation for positive distributions is a log-transform.\"\"\"\n",
    "        return np.log(x)\n",
    "\n",
    "    def inv_transform(self, x):\n",
    "        return np.exp(x)\n",
    "\n",
    "\n",
    "class Normal(Distribution):\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dist = norm(loc=mu, scale=sigma)\n",
    "\n",
    "\n",
    "class Exponential(PositiveDistribution):\n",
    "    def __init__(self, lam):\n",
    "        self.lam = lam\n",
    "        self.dist = expon(scale=lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run some simple sanity checks to make sure everything was implemented correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we are going to calculate the log_prob of data for the exponential distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.exponential(scale=1.0, size=(10))\n",
    "\n",
    "X = Exponential(lam=1)\n",
    "X.sumlogpdf(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try generating data from a normal distribution, and call on the transformed sumlogpdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(loc=0.0, scale=1.0, size=(10,))\n",
    "X = Exponential(lam=1)\n",
    "X.sumlogpdf_tfm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try generating data from a normal distribution, and call on the transformed and non-transformed sumlogpdf of the Normal distribution. The results should be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(loc=0, scale=1, size=10)\n",
    "X = Normal(mu=0, sigma=1)\n",
    "X.sumlogpdf(x), X.sumlogpdf_tfm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we're close to finishing the components needed in the distributions library of a PPL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another component we might need to worry about is that when we \"pass\" a distribution into another distribution as a parameter, what we are really doing is sampling a value from that distribution and then passing it into the next.\n",
    "\n",
    "As an example the following model in equations:\n",
    "\n",
    "$$\\mu \\sim N(0, 1)$$\n",
    "\n",
    "$$L \\sim N(\\mu, 1)$$\n",
    "\n",
    "translates to the following code (while simulating data from a model not \"fitted\" to data):\n",
    "\n",
    "```python\n",
    "mu = Normal(0, 1)\n",
    "L = Normal(mu.sample(1), 1)\n",
    "```\n",
    "\n",
    "However, while in inference mode, that is, when trying to sample posterior values of `mu`, we instead need the above code to be transformed into the following pseudocode:\n",
    "\n",
    "```python\n",
    "loglike_mu(mu) + loglike_L(mu, L, data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're going to see whether we can use this to build a model that can return a log-probability function. That is basically what a model has to do.\n",
    "\n",
    "But first, some ideas to chew on.\n",
    "\n",
    "**Firstly**, the model has to know what its random variables, likelihood, and data are. It's possible to have more than one random variable, more than one likelihood, and hence more than one data source passed in. Knowing this implies that there is \"state\" involved, and hence a class-based implementation will make things easier to track than a function-based implementation.\n",
    "\n",
    "**Secondly**, the model has to know which random variables are passed into the likelihood function. Only then we can construct the evaluation of the likelihood correctly. To remind ourselves on why:\n",
    "\n",
    "```python\n",
    "def model_loglike(mu, sigma, xs):\n",
    "    return normal_prior_loglike(mu) + sigma_prior_loglike(sigma) + data_loglike(mu, sigma, xs)\n",
    "```\n",
    "\n",
    "Note here how `mu` has to be passed into the `normal_prior_loglike` and the `data_loglike`, while `sigma` has to be passed into the `sigma_prior_loglike` and the `data_loglike`. This induces a \"graph\" of sorts, which reveals to us another concept: Bayesian models have a \"graphical\" representation, through which data and model parameters flow through from the priors to the likelihood.\n",
    "\n",
    "**Thirdly**, for diagnostic purposes, we need to be able to do a \"prior sampling\" step, that is to say, simulate what data would look like if generated from our priors. In the absence of data, this step lets us calibrate our priors better: we could impose soft constraints (e.g. by tuning variance parameters) to ensure that the range of our data fall within the right order of mangitude(s) before fitting. We thus need to have an API that has something like `model.prior_samples()`, which then returns distributions for each of the latent parameters and simulated data from the model likelihood.\n",
    "\n",
    "**Fourthly**, again for diagnostic purposes, we need to be able to do a \"posterior sampling\" step, that is to simulate what the data looks like under the posterior sampled parameters. The steps involved here are to take each of the sampled posterior values of the random variables, pass them to the appropriate parts of the log-likelihood function, and generate data from the log-likelihood.\n",
    "\n",
    "**Fifthly**, we need the ability to do "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to decompose this into its constituent components.\n",
    "\n",
    "1. We have a `sampler` to sample from the joint log-likelihood, which should accept proposed random variable values and data, and return a scalar.\n",
    "2. We have a `model` specification, which should accept data and return a log-likelihood function and the random variables values to evaluate.\n",
    "\n",
    "An API sketch might look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian",
   "language": "python",
   "name": "bayesian"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
