{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Bayesian Analysis Recipes Introduction I've recently been inspired by how flexible and powerful Bayesian statistical analysis can be. Yet, as with many things, flexibility often means a tradeoff with ease-of-use. I think having a cookbook of code that can be used in a number of settings can be extremely helpful for bringing Bayesian methods to a more general setting! Notebooks There is one notebook per model. In each notebook, you should end up finding: The kind of problem that is being tackled here. A description of how the data should be structured. An example data table. It generally will end up being tidy data. PyMC3 code for the model; in some notebooks, there may be two versions of the same model. Examples on how to report findings from the MCMC-sampled posterior. It is my hope that these recipes will be useful for you! (hypo)thesis My hypothesis here follows the Pareto principle: a large fraction of real-world problems can essentially be modelled with a core collection of models, each of which have a Bayesian interpretation. In particular, I have this hunch that commonly-used methods like ANOVA can be replaced by conceptually simpler and much more interpretable Bayesian alternatives, like John Kruschke's BEST ( B ayesian E stimation S upersedes the T -test). For example, ANOVA only tests whether means of multiple treatment groups are the same or not... but BEST gives us the estimated posterior distribution over each of the treatment groups, assuming each treatment group is i.i.d. Hence, richer information can be gleaned: we can, given the data at hand, make statements about how any particular pair of groups are different, without requiring additional steps such as multiple hypothesis corrections. Further reading/watching/listening Books: Bayesian Methods for Hackers Think Bayes Papers: Bayesian Estimation Supersedes the t-Test Videos: Computational Statistics I @ SciPy 2015 Computational Statistics II @ SciPy 2015 Bayesian Statistical Analysis with Python @ PyCon 2017 Bayesian Estimation Supersedes the t-Test Got Feedback? There's a few ways you can help make this repository an awesome one for Bayesian method learners out there. If you have a question: Post a GitHub issue with your question. I'll try my best to respond as soon as possible. If you have a suggested change: Submit a pull request detailing the change and why you think it's important. Keep it simple, no need to have essay-length justifications; this also makes things easier for me to review. As usual, I will get back to you as soon as I can.","title":"Bayesian Analysis Recipes"},{"location":"#bayesian-analysis-recipes","text":"","title":"Bayesian Analysis Recipes"},{"location":"#introduction","text":"I've recently been inspired by how flexible and powerful Bayesian statistical analysis can be. Yet, as with many things, flexibility often means a tradeoff with ease-of-use. I think having a cookbook of code that can be used in a number of settings can be extremely helpful for bringing Bayesian methods to a more general setting!","title":"Introduction"},{"location":"#notebooks","text":"There is one notebook per model. In each notebook, you should end up finding: The kind of problem that is being tackled here. A description of how the data should be structured. An example data table. It generally will end up being tidy data. PyMC3 code for the model; in some notebooks, there may be two versions of the same model. Examples on how to report findings from the MCMC-sampled posterior. It is my hope that these recipes will be useful for you!","title":"Notebooks"},{"location":"#hypothesis","text":"My hypothesis here follows the Pareto principle: a large fraction of real-world problems can essentially be modelled with a core collection of models, each of which have a Bayesian interpretation. In particular, I have this hunch that commonly-used methods like ANOVA can be replaced by conceptually simpler and much more interpretable Bayesian alternatives, like John Kruschke's BEST ( B ayesian E stimation S upersedes the T -test). For example, ANOVA only tests whether means of multiple treatment groups are the same or not... but BEST gives us the estimated posterior distribution over each of the treatment groups, assuming each treatment group is i.i.d. Hence, richer information can be gleaned: we can, given the data at hand, make statements about how any particular pair of groups are different, without requiring additional steps such as multiple hypothesis corrections.","title":"(hypo)thesis"},{"location":"#further-readingwatchinglistening","text":"Books: Bayesian Methods for Hackers Think Bayes Papers: Bayesian Estimation Supersedes the t-Test Videos: Computational Statistics I @ SciPy 2015 Computational Statistics II @ SciPy 2015 Bayesian Statistical Analysis with Python @ PyCon 2017 Bayesian Estimation Supersedes the t-Test","title":"Further reading/watching/listening"},{"location":"#got-feedback","text":"There's a few ways you can help make this repository an awesome one for Bayesian method learners out there. If you have a question: Post a GitHub issue with your question. I'll try my best to respond as soon as possible. If you have a suggested change: Submit a pull request detailing the change and why you think it's important. Keep it simple, no need to have essay-length justifications; this also makes things easier for me to review. As usual, I will get back to you as soon as I can.","title":"Got Feedback?"},{"location":"notebooks/bayesian-estimation-on-multiple-groups/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Bayesian Estimation on Multiple Groups Problem Type The Bayesian estimation model is widely applicable across a number of scenarios. The classical scenario is when we have an experimental design where there is a control vs. a treatment, and we want to know what the difference is between the two. Here, \"estimation\" is used to estimate the \"true\" value for the control and the \"true\" value for the treatment, and the \"Bayesian\" part refers to the computation of the uncertainty surrounding the parameter. Bayesian estimation's advantages over the classical t-test was first described by John Kruschke (2013). In this notebook, I provide a concise implementation suitable for two-sample and multi-sample inference, with data that don't necessarily fit Gaussian assumptions. Data structure To use it with this model, the data should be structured as such: Each row is one measurement. The columns should indicate, at the minimum: What treatment group the sample belonged to. The measured value. Extensions to the model As of now, the model only samples posterior distributions of measured values. The model, then, may be extended to compute differences in means (sample vs. control) or effect sizes, complete with uncertainty around it. Use pm.Deterministic(...) to ensure that those statistics' posterior distributions, i.e. uncertainty, are also computed. Reporting summarized findings Here are examples of how to summarize the findings. Treatment group A was greater than control by x units (95% HPD: [ lower , upper ]). Treatment group A was higher than control (effect size 95% HPD: [ lower , upper ]). % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' import pymc4 as pm4 import pandas as pd import numpy as np import matplotlib.pyplot as plt from matplotlib.gridspec import GridSpec import janitor as jn from utils import ecdf # Read in the data df = ( pd . read_csv ( \"../../datasets/biofilm.csv\" ) . label_encode ( columns = [ \"isolate\" ]) # encode isolate as labels. . transform_column ( \"normalized_measurement\" , np . log , \"log_normalized_measurement\" ) ) # Display a subset of the data. df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } experiment isolate ST OD600 measurement replicate normalized_measurement isolate_enc log_normalized_measurement 0 1 1 4 0.461 0.317 1 0.687636 0 -0.374496 1 1 2 55 0.346 0.434 1 1.254335 7 0.226606 2 1 3 55 0.356 0.917 1 2.575843 8 0.946177 3 1 4 4 0.603 1.061 1 1.759536 9 0.565050 4 1 5 330 0.444 3.701 1 8.335586 10 2.120534 Model Specification We know that the OD600 and measurements columns are all positive-valued, and so the normalized_measurement column will also be positive-valued. There are two ways to handle this situation: We can either choose to directly model the likelihood using a bounded, positive-support-only distribution, or We can model the log-transformation of the normalized_measurement column, using an unbounded, infinite-support distribution (e.g. the T-distribution family of distributions, which includes the Gaussian and the Cauchy in there). The former is ever slightly more convenient to reason about, but the latter lets us use Gaussians, which have some nice properties when sampling. import tensorflow as tf import numpy as np num_isolates = len ( set ( df [ \"isolate_enc\" ])) @pm4 . model def bacteria_model (): mu_mean = yield pm4 . Normal ( \"mu_mean\" , loc = 0 , scale = 1 ) mu = yield pm4 . Normal ( \"mu\" , loc = mu_mean , scale = 1 , batch_stack = num_isolates ) mu_bounded = yield pm4 . Deterministic ( \"mu_bounded\" , tf . exp ( mu )) # Because we use TFP, tf.gather now replaces the old numpy syntax. # the following line is equivalent to: # mu_all = mu[df[\"isolate_enc\"]] mu_all = tf . gather ( mu , df [ \"isolate_enc\" ]) sigma = yield pm4 . HalfCauchy ( \"sd\" , scale = 1 , batch_stack = num_isolates ) sigma_all = tf . gather ( sigma , df [ \"isolate_enc\" ]) nu = yield pm4 . Exponential ( \"nu\" , rate = 1 / 30. ) like = yield pm4 . StudentT ( \"like\" , loc = mu_all , scale = sigma_all , df = nu , observed = df [ \"log_normalized_measurement\" ]) # Take the difference against the ATCC strain, which is the control. difference = yield pm4 . Deterministic ( \"difference\" , mu_bounded [: - 1 ] - mu_bounded [ - 1 ]) Inference Button! Now, we hit the Inference Button(tm) and sample from the posterior distribution. trace = pm4 . sample ( bacteria_model ()) Diagnostics Our first diagnostic will be the trace plot. We expect the trace of the variables that we are most interested in to be a fuzzy caterpillar. import arviz as az axes = az . plot_trace ( trace , var_names = [ \"bacteria_model/mu\" ], compact = True ) Looking at the traces, yes, everything looks more or less like a hairy caterpillar. This means that sampling went well, and has converged, thus we have a good MCMC estimator of the posterior distribution. I need a mapping of isolate to its encoding - will come in handy below. mapping = dict ( zip ( df [ \"isolate_enc\" ], df [ \"isolate\" ])) yticklabels = list ( reversed ([ mapping [ i ] for i in range ( len ( mapping ))])) Let's now plot the posterior distributions. We'll use a ridge plot, as it's both aesthetically pleasing and informative. fig , ax = plt . subplots ( figsize = ( 8 , 6 )) axes = az . plot_forest ( trace , var_names = [ \"bacteria_model/mu_bounded\" ], ax = ax , kind = \"ridgeplot\" ) axes [ 0 ] . set_yticklabels ( yticklabels ); On the basis of this, we would say that strain 5 was the most different from the other strains. Let's now look at the differences directly. fig , ax = plt . subplots ( figsize = ( 8 , 6 )) axes = az . plot_forest ( trace , var_names = [ \"bacteria_model/difference\" ], ax = ax , kind = \"ridgeplot\" ) axes [ 0 ] . axvline ( 0 , color = \"black\" ) axes [ 0 ] . set_yticklabels ( yticklabels [ 1 :]); If we were in a binary decision-making mode, we would say that isolates 5 was the most \"significantly\" different from the ATCC strain. trace_with_posterior = pm4 . sample_posterior_predictive ( bacteria_model (), trace = trace ) WARNING:tensorflow:AutoGraph could not transform <function pfor.<locals>.f at 0x7ff8ec083ee0> and will run it as-is. Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Inconsistent ASTs detected. This is a bug. Cause: inconsistent values for field args: [<gast.gast.Name object at 0x7ff8e406aee0>, <gast.gast.Name object at 0x7ff8e406a910>, <gast.gast.Name object at 0x7ff8e406ad30>] and []Diff: *** Original nodes --- Reparsed nodes *************** *** 41,55 **** | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" --- 41,56 ---- | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[] ! | | | | posonlyargs=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" *************** *** 58,72 **** | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_module__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | ] - | | | | posonlyargs=[] | | | | vararg=None | | | | kwonlyargs=[] | | | | kw_defaults=[] | | | | kwarg=None | | | | defaults=[] | | | body=[ | | | | FunctionDef: --- 59,72 ---- To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert WARNING: AutoGraph could not transform <function pfor.<locals>.f at 0x7ff8ec083ee0> and will run it as-is. Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Inconsistent ASTs detected. This is a bug. Cause: inconsistent values for field args: [<gast.gast.Name object at 0x7ff8e406aee0>, <gast.gast.Name object at 0x7ff8e406a910>, <gast.gast.Name object at 0x7ff8e406ad30>] and []Diff: *** Original nodes --- Reparsed nodes *************** *** 41,55 **** | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" --- 41,56 ---- | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[] ! | | | | posonlyargs=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" *************** *** 58,72 **** | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_module__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | ] - | | | | posonlyargs=[] | | | | vararg=None | | | | kwonlyargs=[] | | | | kw_defaults=[] | | | | kwarg=None | | | | defaults=[] | | | body=[ | | | | FunctionDef: --- 59,72 ---- To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert WARNING:tensorflow:AutoGraph could not transform <function _convert_function_call.<locals>.f at 0x7ff8ac68fc10> and will run it as-is. Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Inconsistent ASTs detected. This is a bug. Cause: inconsistent values for field args: [<gast.gast.Name object at 0x7ff9402aaca0>] and []Diff: *** Original nodes --- Reparsed nodes *************** *** 30,44 **** | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" --- 30,45 ---- | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[] ! | | | | posonlyargs=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" *************** *** 47,61 **** | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_module__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | ] - | | | | posonlyargs=[] | | | | vararg=None | | | | kwonlyargs=[] | | | | kw_defaults=[] | | | | kwarg=None | | | | defaults=[] | | | body=[ | | | | FunctionDef: --- 48,61 ---- *************** *** 372,415 **** | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"set_state\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"loop_vars\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] - | | | | | | | | | | posonlyargs=[] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ ! | | | | | | | | | | Pass() | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"loop_body\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"itr\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] - | | | | | | | | | | posonlyargs=[] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ | | | | | | | | | | Assign: --- 372,415 ---- | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"set_state\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[] ! | | | | | | | | | | posonlyargs=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"loop_vars\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ ! | | | | | | | | | | Pass: | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"loop_body\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[] ! | | | | | | | | | | posonlyargs=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"itr\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ | | | | | | | | | | Assign: To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert WARNING: AutoGraph could not transform <function _convert_function_call.<locals>.f at 0x7ff8ac68fc10> and will run it as-is. Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Inconsistent ASTs detected. This is a bug. Cause: inconsistent values for field args: [<gast.gast.Name object at 0x7ff9402aaca0>] and []Diff: *** Original nodes --- Reparsed nodes *************** *** 30,44 **** | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" --- 30,45 ---- | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[] ! | | | | posonlyargs=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" *************** *** 47,61 **** | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_module__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | ] - | | | | posonlyargs=[] | | | | vararg=None | | | | kwonlyargs=[] | | | | kw_defaults=[] | | | | kwarg=None | | | | defaults=[] | | | body=[ | | | | FunctionDef: --- 48,61 ---- *************** *** 372,415 **** | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"set_state\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"loop_vars\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] - | | | | | | | | | | posonlyargs=[] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ ! | | | | | | | | | | Pass() | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"loop_body\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"itr\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] - | | | | | | | | | | posonlyargs=[] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ | | | | | | | | | | Assign: --- 372,415 ---- | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"set_state\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[] ! | | | | | | | | | | posonlyargs=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"loop_vars\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ ! | | | | | | | | | | Pass: | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"loop_body\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[] ! | | | | | | | | | | posonlyargs=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"itr\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ | | | | | | | | | | Assign: To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert WARNING:tensorflow:Note that RandomGamma inside pfor op may not give same output as inside a sequential loop. WARNING:tensorflow:Note that RandomStandardNormal inside pfor op may not give same output as inside a sequential loop. trace . posterior_predictive --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-11-804945b1ab07> in <module> ----> 1 trace . posterior_predictive AttributeError : 'InferenceData' object has no attribute 'posterior_predictive' # We want indices for each of the samples. indices = dict () for enc , iso in mapping . items (): idxs = list ( df [ df [ \"isolate_enc\" ] == enc ] . index ) indices [ iso ] = idxs indices # Make PPC plot for one of the groups. fig = plt . figure ( figsize = ( 16 , 16 )) gs = GridSpec ( nrows = 4 , ncols = 4 ) axes = dict () for i , ( strain , idxs ) in enumerate ( indices . items ()): if i > 0 : ax = fig . add_subplot ( gs [ i ], sharex = axes [ 0 ]) else : ax = fig . add_subplot ( gs [ i ]) x , y = ecdf ( df . iloc [ idxs ][ \"log_normalized_measurement\" ]) ax . plot ( x , y , label = \"data\" ) x , y = ecdf ( trace . posterior_predictive [ \"bacteria_model/like\" ] . loc [:, :, idxs ] . mean ( axis = ( 2 )) . data . flatten () ) ax . plot ( x , y , label = \"ppc\" ) ax . set_title ( f \"Strain { strain } \" ) axes [ i ] = ax The PPC draws clearly have longer tails than do the originals. I chalk this down to having small number of samples. The central tendency is definitely modelled well, and I don't see wild deviations between the sampled posterior and the measured data.","title":"Bayesian estimation on multiple groups"},{"location":"notebooks/bayesian-estimation-on-multiple-groups/#bayesian-estimation-on-multiple-groups","text":"","title":"Bayesian Estimation on Multiple Groups"},{"location":"notebooks/bayesian-estimation-on-multiple-groups/#problem-type","text":"The Bayesian estimation model is widely applicable across a number of scenarios. The classical scenario is when we have an experimental design where there is a control vs. a treatment, and we want to know what the difference is between the two. Here, \"estimation\" is used to estimate the \"true\" value for the control and the \"true\" value for the treatment, and the \"Bayesian\" part refers to the computation of the uncertainty surrounding the parameter. Bayesian estimation's advantages over the classical t-test was first described by John Kruschke (2013). In this notebook, I provide a concise implementation suitable for two-sample and multi-sample inference, with data that don't necessarily fit Gaussian assumptions.","title":"Problem Type"},{"location":"notebooks/bayesian-estimation-on-multiple-groups/#data-structure","text":"To use it with this model, the data should be structured as such: Each row is one measurement. The columns should indicate, at the minimum: What treatment group the sample belonged to. The measured value.","title":"Data structure"},{"location":"notebooks/bayesian-estimation-on-multiple-groups/#extensions-to-the-model","text":"As of now, the model only samples posterior distributions of measured values. The model, then, may be extended to compute differences in means (sample vs. control) or effect sizes, complete with uncertainty around it. Use pm.Deterministic(...) to ensure that those statistics' posterior distributions, i.e. uncertainty, are also computed.","title":"Extensions to the model"},{"location":"notebooks/bayesian-estimation-on-multiple-groups/#reporting-summarized-findings","text":"Here are examples of how to summarize the findings. Treatment group A was greater than control by x units (95% HPD: [ lower , upper ]). Treatment group A was higher than control (effect size 95% HPD: [ lower , upper ]). % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' import pymc4 as pm4 import pandas as pd import numpy as np import matplotlib.pyplot as plt from matplotlib.gridspec import GridSpec import janitor as jn from utils import ecdf # Read in the data df = ( pd . read_csv ( \"../../datasets/biofilm.csv\" ) . label_encode ( columns = [ \"isolate\" ]) # encode isolate as labels. . transform_column ( \"normalized_measurement\" , np . log , \"log_normalized_measurement\" ) ) # Display a subset of the data. df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } experiment isolate ST OD600 measurement replicate normalized_measurement isolate_enc log_normalized_measurement 0 1 1 4 0.461 0.317 1 0.687636 0 -0.374496 1 1 2 55 0.346 0.434 1 1.254335 7 0.226606 2 1 3 55 0.356 0.917 1 2.575843 8 0.946177 3 1 4 4 0.603 1.061 1 1.759536 9 0.565050 4 1 5 330 0.444 3.701 1 8.335586 10 2.120534","title":"Reporting summarized findings"},{"location":"notebooks/bayesian-estimation-on-multiple-groups/#model-specification","text":"We know that the OD600 and measurements columns are all positive-valued, and so the normalized_measurement column will also be positive-valued. There are two ways to handle this situation: We can either choose to directly model the likelihood using a bounded, positive-support-only distribution, or We can model the log-transformation of the normalized_measurement column, using an unbounded, infinite-support distribution (e.g. the T-distribution family of distributions, which includes the Gaussian and the Cauchy in there). The former is ever slightly more convenient to reason about, but the latter lets us use Gaussians, which have some nice properties when sampling. import tensorflow as tf import numpy as np num_isolates = len ( set ( df [ \"isolate_enc\" ])) @pm4 . model def bacteria_model (): mu_mean = yield pm4 . Normal ( \"mu_mean\" , loc = 0 , scale = 1 ) mu = yield pm4 . Normal ( \"mu\" , loc = mu_mean , scale = 1 , batch_stack = num_isolates ) mu_bounded = yield pm4 . Deterministic ( \"mu_bounded\" , tf . exp ( mu )) # Because we use TFP, tf.gather now replaces the old numpy syntax. # the following line is equivalent to: # mu_all = mu[df[\"isolate_enc\"]] mu_all = tf . gather ( mu , df [ \"isolate_enc\" ]) sigma = yield pm4 . HalfCauchy ( \"sd\" , scale = 1 , batch_stack = num_isolates ) sigma_all = tf . gather ( sigma , df [ \"isolate_enc\" ]) nu = yield pm4 . Exponential ( \"nu\" , rate = 1 / 30. ) like = yield pm4 . StudentT ( \"like\" , loc = mu_all , scale = sigma_all , df = nu , observed = df [ \"log_normalized_measurement\" ]) # Take the difference against the ATCC strain, which is the control. difference = yield pm4 . Deterministic ( \"difference\" , mu_bounded [: - 1 ] - mu_bounded [ - 1 ])","title":"Model Specification"},{"location":"notebooks/bayesian-estimation-on-multiple-groups/#inference-button","text":"Now, we hit the Inference Button(tm) and sample from the posterior distribution. trace = pm4 . sample ( bacteria_model ())","title":"Inference Button!"},{"location":"notebooks/bayesian-estimation-on-multiple-groups/#diagnostics","text":"Our first diagnostic will be the trace plot. We expect the trace of the variables that we are most interested in to be a fuzzy caterpillar. import arviz as az axes = az . plot_trace ( trace , var_names = [ \"bacteria_model/mu\" ], compact = True ) Looking at the traces, yes, everything looks more or less like a hairy caterpillar. This means that sampling went well, and has converged, thus we have a good MCMC estimator of the posterior distribution. I need a mapping of isolate to its encoding - will come in handy below. mapping = dict ( zip ( df [ \"isolate_enc\" ], df [ \"isolate\" ])) yticklabels = list ( reversed ([ mapping [ i ] for i in range ( len ( mapping ))])) Let's now plot the posterior distributions. We'll use a ridge plot, as it's both aesthetically pleasing and informative. fig , ax = plt . subplots ( figsize = ( 8 , 6 )) axes = az . plot_forest ( trace , var_names = [ \"bacteria_model/mu_bounded\" ], ax = ax , kind = \"ridgeplot\" ) axes [ 0 ] . set_yticklabels ( yticklabels ); On the basis of this, we would say that strain 5 was the most different from the other strains. Let's now look at the differences directly. fig , ax = plt . subplots ( figsize = ( 8 , 6 )) axes = az . plot_forest ( trace , var_names = [ \"bacteria_model/difference\" ], ax = ax , kind = \"ridgeplot\" ) axes [ 0 ] . axvline ( 0 , color = \"black\" ) axes [ 0 ] . set_yticklabels ( yticklabels [ 1 :]); If we were in a binary decision-making mode, we would say that isolates 5 was the most \"significantly\" different from the ATCC strain. trace_with_posterior = pm4 . sample_posterior_predictive ( bacteria_model (), trace = trace ) WARNING:tensorflow:AutoGraph could not transform <function pfor.<locals>.f at 0x7ff8ec083ee0> and will run it as-is. Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Inconsistent ASTs detected. This is a bug. Cause: inconsistent values for field args: [<gast.gast.Name object at 0x7ff8e406aee0>, <gast.gast.Name object at 0x7ff8e406a910>, <gast.gast.Name object at 0x7ff8e406ad30>] and []Diff: *** Original nodes --- Reparsed nodes *************** *** 41,55 **** | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" --- 41,56 ---- | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[] ! | | | | posonlyargs=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" *************** *** 58,72 **** | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_module__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | ] - | | | | posonlyargs=[] | | | | vararg=None | | | | kwonlyargs=[] | | | | kw_defaults=[] | | | | kwarg=None | | | | defaults=[] | | | body=[ | | | | FunctionDef: --- 59,72 ---- To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert WARNING: AutoGraph could not transform <function pfor.<locals>.f at 0x7ff8ec083ee0> and will run it as-is. Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Inconsistent ASTs detected. This is a bug. Cause: inconsistent values for field args: [<gast.gast.Name object at 0x7ff8e406aee0>, <gast.gast.Name object at 0x7ff8e406a910>, <gast.gast.Name object at 0x7ff8e406ad30>] and []Diff: *** Original nodes --- Reparsed nodes *************** *** 41,55 **** | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" --- 41,56 ---- | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[] ! | | | | posonlyargs=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" *************** *** 58,72 **** | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_module__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | ] - | | | | posonlyargs=[] | | | | vararg=None | | | | kwonlyargs=[] | | | | kw_defaults=[] | | | | kwarg=None | | | | defaults=[] | | | body=[ | | | | FunctionDef: --- 59,72 ---- To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert WARNING:tensorflow:AutoGraph could not transform <function _convert_function_call.<locals>.f at 0x7ff8ac68fc10> and will run it as-is. Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Inconsistent ASTs detected. This is a bug. Cause: inconsistent values for field args: [<gast.gast.Name object at 0x7ff9402aaca0>] and []Diff: *** Original nodes --- Reparsed nodes *************** *** 30,44 **** | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" --- 30,45 ---- | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[] ! | | | | posonlyargs=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" *************** *** 47,61 **** | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_module__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | ] - | | | | posonlyargs=[] | | | | vararg=None | | | | kwonlyargs=[] | | | | kw_defaults=[] | | | | kwarg=None | | | | defaults=[] | | | body=[ | | | | FunctionDef: --- 48,61 ---- *************** *** 372,415 **** | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"set_state\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"loop_vars\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] - | | | | | | | | | | posonlyargs=[] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ ! | | | | | | | | | | Pass() | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"loop_body\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"itr\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] - | | | | | | | | | | posonlyargs=[] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ | | | | | | | | | | Assign: --- 372,415 ---- | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"set_state\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[] ! | | | | | | | | | | posonlyargs=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"loop_vars\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ ! | | | | | | | | | | Pass: | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"loop_body\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[] ! | | | | | | | | | | posonlyargs=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"itr\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ | | | | | | | | | | Assign: To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert WARNING: AutoGraph could not transform <function _convert_function_call.<locals>.f at 0x7ff8ac68fc10> and will run it as-is. Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Inconsistent ASTs detected. This is a bug. Cause: inconsistent values for field args: [<gast.gast.Name object at 0x7ff9402aaca0>] and []Diff: *** Original nodes --- Reparsed nodes *************** *** 30,44 **** | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" --- 30,45 ---- | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[] ! | | | | posonlyargs=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" *************** *** 47,61 **** | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_module__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | ] - | | | | posonlyargs=[] | | | | vararg=None | | | | kwonlyargs=[] | | | | kw_defaults=[] | | | | kwarg=None | | | | defaults=[] | | | body=[ | | | | FunctionDef: --- 48,61 ---- *************** *** 372,415 **** | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"set_state\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"loop_vars\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] - | | | | | | | | | | posonlyargs=[] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ ! | | | | | | | | | | Pass() | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"loop_body\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"itr\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] - | | | | | | | | | | posonlyargs=[] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ | | | | | | | | | | Assign: --- 372,415 ---- | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"set_state\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[] ! | | | | | | | | | | posonlyargs=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"loop_vars\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ ! | | | | | | | | | | Pass: | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"loop_body\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[] ! | | | | | | | | | | posonlyargs=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"itr\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ | | | | | | | | | | Assign: To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert WARNING:tensorflow:Note that RandomGamma inside pfor op may not give same output as inside a sequential loop. WARNING:tensorflow:Note that RandomStandardNormal inside pfor op may not give same output as inside a sequential loop. trace . posterior_predictive --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-11-804945b1ab07> in <module> ----> 1 trace . posterior_predictive AttributeError : 'InferenceData' object has no attribute 'posterior_predictive' # We want indices for each of the samples. indices = dict () for enc , iso in mapping . items (): idxs = list ( df [ df [ \"isolate_enc\" ] == enc ] . index ) indices [ iso ] = idxs indices # Make PPC plot for one of the groups. fig = plt . figure ( figsize = ( 16 , 16 )) gs = GridSpec ( nrows = 4 , ncols = 4 ) axes = dict () for i , ( strain , idxs ) in enumerate ( indices . items ()): if i > 0 : ax = fig . add_subplot ( gs [ i ], sharex = axes [ 0 ]) else : ax = fig . add_subplot ( gs [ i ]) x , y = ecdf ( df . iloc [ idxs ][ \"log_normalized_measurement\" ]) ax . plot ( x , y , label = \"data\" ) x , y = ecdf ( trace . posterior_predictive [ \"bacteria_model/like\" ] . loc [:, :, idxs ] . mean ( axis = ( 2 )) . data . flatten () ) ax . plot ( x , y , label = \"ppc\" ) ax . set_title ( f \"Strain { strain } \" ) axes [ i ] = ax The PPC draws clearly have longer tails than do the originals. I chalk this down to having small number of samples. The central tendency is definitely modelled well, and I don't see wild deviations between the sampled posterior and the measured data.","title":"Diagnostics"},{"location":"notebooks/degrees-of-freedom/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Purpose Just testing my intuition w.r.t. degrees of freedom in the students T distribution. Cauchy: df = 1. Normal: df = infinity (or at least some really high number) This should be reflected when using PyMC3. import pymc3 as pm import numpy as np import matplotlib.pyplot as plt import arviz as az % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload normal = np . random . normal ( size = 20000 ) cauchy = np . random . standard_cauchy ( size = 20000 ) with pm . Model () as normal_model : mu = pm . Normal ( \"mu\" , mu = 0 , sd = 100 ) sd = pm . HalfNormal ( \"sd\" , sd = 100 ) nu = pm . Exponential ( \"nu\" , lam = 0.5 ) like = pm . StudentT ( \"like\" , mu = mu , sd = sd , nu = nu , observed = normal ) trace = pm . sample ( 2000 , tune = 2000 ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [nu, sd, mu] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16000/16000 [00:20<00:00, 790.93draws/s] axes = az . plot_trace ( trace ) Many degrees of freedom for normal distribution. Makes sense. with pm . Model () as cauchy_model : mu = pm . Normal ( \"mu\" , mu = 0 , sd = 100 ) sd = pm . HalfNormal ( \"sd\" , sd = 100 ) nu = pm . Exponential ( \"nu\" , lam = 1 ) like = pm . StudentT ( \"like\" , mu = mu , sd = sd , nu = nu , observed = cauchy ) trace = pm . sample ( 2000 , tune = 2000 ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [nu, sd, mu] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16000/16000 [00:22<00:00, 709.93draws/s] axes = az . plot_trace ( trace ) Basically 1 degree of freedom when inferring \\nu \\nu from Cauchy-distributed data. Yes :).","title":"Degrees of freedom"},{"location":"notebooks/degrees-of-freedom/#purpose","text":"Just testing my intuition w.r.t. degrees of freedom in the students T distribution. Cauchy: df = 1. Normal: df = infinity (or at least some really high number) This should be reflected when using PyMC3. import pymc3 as pm import numpy as np import matplotlib.pyplot as plt import arviz as az % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload normal = np . random . normal ( size = 20000 ) cauchy = np . random . standard_cauchy ( size = 20000 ) with pm . Model () as normal_model : mu = pm . Normal ( \"mu\" , mu = 0 , sd = 100 ) sd = pm . HalfNormal ( \"sd\" , sd = 100 ) nu = pm . Exponential ( \"nu\" , lam = 0.5 ) like = pm . StudentT ( \"like\" , mu = mu , sd = sd , nu = nu , observed = normal ) trace = pm . sample ( 2000 , tune = 2000 ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [nu, sd, mu] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16000/16000 [00:20<00:00, 790.93draws/s] axes = az . plot_trace ( trace ) Many degrees of freedom for normal distribution. Makes sense. with pm . Model () as cauchy_model : mu = pm . Normal ( \"mu\" , mu = 0 , sd = 100 ) sd = pm . HalfNormal ( \"sd\" , sd = 100 ) nu = pm . Exponential ( \"nu\" , lam = 1 ) like = pm . StudentT ( \"like\" , mu = mu , sd = sd , nu = nu , observed = cauchy ) trace = pm . sample ( 2000 , tune = 2000 ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [nu, sd, mu] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16000/16000 [00:22<00:00, 709.93draws/s] axes = az . plot_trace ( trace ) Basically 1 degree of freedom when inferring \\nu \\nu from Cauchy-distributed data. Yes :).","title":"Purpose"},{"location":"notebooks/dirichlet-multinomial-bayesian-proportions/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Introduction Let's say there are three bacteria species that characterize the gut, and we hypothesize that they are ever so shifted off from one another, but we don't know how (i.e. ignore the data-generating distribution below). Can we figure out the proportion parameters and their uncertainty? Generate Synthetic Data In the synthetic dataset generated below, we pretend that every patient is one sample, and we are recording the number of sequencing reads corresponding to some OTUs (bacteria). Each row is one sample (patient), and each column is one OTU (sample). Proportions Firstly, let's generate the ground truth proportions that we will infer later on. import numpy as np import pandas as pd import matplotlib.pyplot as plt import pymc3 as pm import numpy.random as npr % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl' def proportion ( arr ): arr = np . asarray ( arr ) return arr / arr . sum () healthy_proportions = proportion ([ 10 , 16 , 2 ]) healthy_proportions array([0.35714286, 0.57142857, 0.07142857]) sick_proportions = proportion ([ 10 , 27 , 15 ]) sick_proportions array([0.19230769, 0.51923077, 0.28846154]) Data Now, given the proportions, let's generate data. Here, we are assuming that there are 10 patients per cohort (10 sick patients and 10 healthy patients), and that the number of counts in total is 50. n_data_points = 10 def make_healthy_multinomial ( arr ): n_sequencing_reads = 50 # npr.poisson(lam=50) return npr . multinomial ( n_sequencing_reads , healthy_proportions ) def make_sick_multinomial ( arr ): n_sequencing_reads = 50 # npr.poisson(lam=50) return npr . multinomial ( n_sequencing_reads , sick_proportions ) # Generate healthy data healthy_reads = np . zeros (( n_data_points , 3 )) healthy_reads = np . apply_along_axis ( make_healthy_multinomial , axis = 1 , arr = healthy_reads ) # Generate sick reads sick_reads = np . zeros (( n_data_points , 3 )) sick_reads = np . apply_along_axis ( make_sick_multinomial , axis = 1 , arr = sick_reads ) # Make pandas dataframe healthy_df = pd . DataFrame ( healthy_reads ) healthy_df . columns = [ \"bacteria1\" , \"bacteria2\" , \"bacteria3\" ] healthy_df = pm . floatX ( healthy_df ) sick_df = pd . DataFrame ( sick_reads ) sick_df . columns = [ \"bacteria1\" , \"bacteria2\" , \"bacteria3\" ] sick_df = pm . floatX ( sick_df ) healthy_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bacteria1 bacteria2 bacteria3 0 18.0 28.0 4.0 1 16.0 33.0 1.0 2 19.0 30.0 1.0 3 15.0 31.0 4.0 4 17.0 29.0 4.0 5 16.0 26.0 8.0 6 19.0 29.0 2.0 7 16.0 27.0 7.0 8 21.0 25.0 4.0 9 21.0 27.0 2.0 sick_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bacteria1 bacteria2 bacteria3 0 3.0 29.0 18.0 1 13.0 20.0 17.0 2 11.0 25.0 14.0 3 7.0 27.0 16.0 4 8.0 29.0 13.0 5 11.0 23.0 16.0 6 5.0 28.0 17.0 7 7.0 25.0 18.0 8 13.0 20.0 17.0 9 9.0 25.0 16.0 Model Construction Here's an implementation of the model - Dirichlet prior with Multinomial likelihood. There are 3 classes of bacteria, so the Dirichlet distribution serves as the prior probability mass over each of the classes in the multinomial distribution. The multinomial distribution serves as the likelihood function. with pm . Model () as dirichlet_model : proportions_healthy = pm . Dirichlet ( \"proportions_healthy\" , a = np . array ([ 1.0 ] * 3 ) . astype ( \"float32\" ), shape = ( 3 ,), testval = [ 0.1 , 0.1 , 0.1 ], ) proportions_sick = pm . Dirichlet ( \"proportions_sick\" , a = np . array ([ 1.0 ] * 3 ) . astype ( \"float32\" ), shape = ( 3 ,), testval = [ 0.1 , 0.1 , 0.1 ], ) healthy_like = pm . Multinomial ( \"like_healthy\" , n = 50 , p = proportions_healthy , observed = healthy_df . values ) sick_like = pm . Multinomial ( \"like_sick\" , n = 50 , p = proportions_sick , observed = sick_df . values ) Sampling import arviz as az with dirichlet_model : dirichlet_trace = pm . sample ( 2000 ) az . plot_trace ( dirichlet_trace ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [proportions_sick, proportions_healthy] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:03<00:00, 3301.52draws/s] Results ylabels = [ \"healthy_bacteria1\" , \"healthy_bacteria2\" , \"healthy_bacteria3\" , \"sick_bacteria1\" , \"sick_bacteria2\" , \"sick_bacteria3\" , ] axes = az . plot_forest ( dirichlet_trace ) healthy_proportions , sick_proportions (array([0.35714286, 0.57142857, 0.07142857]), array([0.19230769, 0.51923077, 0.28846154])) They match up with the original synthetic percentages!","title":"Dirichlet multinomial bayesian proportions"},{"location":"notebooks/dirichlet-multinomial-bayesian-proportions/#introduction","text":"Let's say there are three bacteria species that characterize the gut, and we hypothesize that they are ever so shifted off from one another, but we don't know how (i.e. ignore the data-generating distribution below). Can we figure out the proportion parameters and their uncertainty?","title":"Introduction"},{"location":"notebooks/dirichlet-multinomial-bayesian-proportions/#generate-synthetic-data","text":"In the synthetic dataset generated below, we pretend that every patient is one sample, and we are recording the number of sequencing reads corresponding to some OTUs (bacteria). Each row is one sample (patient), and each column is one OTU (sample).","title":"Generate Synthetic Data"},{"location":"notebooks/dirichlet-multinomial-bayesian-proportions/#proportions","text":"Firstly, let's generate the ground truth proportions that we will infer later on. import numpy as np import pandas as pd import matplotlib.pyplot as plt import pymc3 as pm import numpy.random as npr % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl' def proportion ( arr ): arr = np . asarray ( arr ) return arr / arr . sum () healthy_proportions = proportion ([ 10 , 16 , 2 ]) healthy_proportions array([0.35714286, 0.57142857, 0.07142857]) sick_proportions = proportion ([ 10 , 27 , 15 ]) sick_proportions array([0.19230769, 0.51923077, 0.28846154])","title":"Proportions"},{"location":"notebooks/dirichlet-multinomial-bayesian-proportions/#data","text":"Now, given the proportions, let's generate data. Here, we are assuming that there are 10 patients per cohort (10 sick patients and 10 healthy patients), and that the number of counts in total is 50. n_data_points = 10 def make_healthy_multinomial ( arr ): n_sequencing_reads = 50 # npr.poisson(lam=50) return npr . multinomial ( n_sequencing_reads , healthy_proportions ) def make_sick_multinomial ( arr ): n_sequencing_reads = 50 # npr.poisson(lam=50) return npr . multinomial ( n_sequencing_reads , sick_proportions ) # Generate healthy data healthy_reads = np . zeros (( n_data_points , 3 )) healthy_reads = np . apply_along_axis ( make_healthy_multinomial , axis = 1 , arr = healthy_reads ) # Generate sick reads sick_reads = np . zeros (( n_data_points , 3 )) sick_reads = np . apply_along_axis ( make_sick_multinomial , axis = 1 , arr = sick_reads ) # Make pandas dataframe healthy_df = pd . DataFrame ( healthy_reads ) healthy_df . columns = [ \"bacteria1\" , \"bacteria2\" , \"bacteria3\" ] healthy_df = pm . floatX ( healthy_df ) sick_df = pd . DataFrame ( sick_reads ) sick_df . columns = [ \"bacteria1\" , \"bacteria2\" , \"bacteria3\" ] sick_df = pm . floatX ( sick_df ) healthy_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bacteria1 bacteria2 bacteria3 0 18.0 28.0 4.0 1 16.0 33.0 1.0 2 19.0 30.0 1.0 3 15.0 31.0 4.0 4 17.0 29.0 4.0 5 16.0 26.0 8.0 6 19.0 29.0 2.0 7 16.0 27.0 7.0 8 21.0 25.0 4.0 9 21.0 27.0 2.0 sick_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bacteria1 bacteria2 bacteria3 0 3.0 29.0 18.0 1 13.0 20.0 17.0 2 11.0 25.0 14.0 3 7.0 27.0 16.0 4 8.0 29.0 13.0 5 11.0 23.0 16.0 6 5.0 28.0 17.0 7 7.0 25.0 18.0 8 13.0 20.0 17.0 9 9.0 25.0 16.0","title":"Data"},{"location":"notebooks/dirichlet-multinomial-bayesian-proportions/#model-construction","text":"Here's an implementation of the model - Dirichlet prior with Multinomial likelihood. There are 3 classes of bacteria, so the Dirichlet distribution serves as the prior probability mass over each of the classes in the multinomial distribution. The multinomial distribution serves as the likelihood function. with pm . Model () as dirichlet_model : proportions_healthy = pm . Dirichlet ( \"proportions_healthy\" , a = np . array ([ 1.0 ] * 3 ) . astype ( \"float32\" ), shape = ( 3 ,), testval = [ 0.1 , 0.1 , 0.1 ], ) proportions_sick = pm . Dirichlet ( \"proportions_sick\" , a = np . array ([ 1.0 ] * 3 ) . astype ( \"float32\" ), shape = ( 3 ,), testval = [ 0.1 , 0.1 , 0.1 ], ) healthy_like = pm . Multinomial ( \"like_healthy\" , n = 50 , p = proportions_healthy , observed = healthy_df . values ) sick_like = pm . Multinomial ( \"like_sick\" , n = 50 , p = proportions_sick , observed = sick_df . values )","title":"Model Construction"},{"location":"notebooks/dirichlet-multinomial-bayesian-proportions/#sampling","text":"import arviz as az with dirichlet_model : dirichlet_trace = pm . sample ( 2000 ) az . plot_trace ( dirichlet_trace ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [proportions_sick, proportions_healthy] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:03<00:00, 3301.52draws/s]","title":"Sampling"},{"location":"notebooks/dirichlet-multinomial-bayesian-proportions/#results","text":"ylabels = [ \"healthy_bacteria1\" , \"healthy_bacteria2\" , \"healthy_bacteria3\" , \"sick_bacteria1\" , \"sick_bacteria2\" , \"sick_bacteria3\" , ] axes = az . plot_forest ( dirichlet_trace ) healthy_proportions , sick_proportions (array([0.35714286, 0.57142857, 0.07142857]), array([0.19230769, 0.51923077, 0.28846154])) They match up with the original synthetic percentages!","title":"Results"},{"location":"notebooks/hierarchical-baseball/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Introduction This notebook shows how to do hierarchical modelling with Binomially-distributed random variables. Problem Setup Baseball players have many metrics measured for them. Let's say we are on a baseball team, and would like to quantify player performance, one metric being their batting average (defined by how many times a batter hit a pitched ball, divided by the number of times they were up for batting (\"at bat\")). How would you go about this task? We first need some measurements of batting data. To answer this question, we need to have data on the number of time a player has batted and the number of times the player has hit the ball while batting. Let's see an example dataset below. % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' import pandas as pd import pymc3 as pm import matplotlib.pyplot as plt import numpy as np import theano.tensor as tt from pyprojroot import here WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl' df = pd . read_csv ( here () / \"datasets/baseballdb/core/Batting.csv\" ) df [ \"AB\" ] = df [ \"AB\" ] . replace ( 0 , np . nan ) df = df . dropna () df [ \"batting_avg\" ] = df [ \"H\" ] / df [ \"AB\" ] df = df [ df [ \"yearID\" ] >= 2016 ] df = df . iloc [ 0 : 15 ] # select out only the first 15 players, just for illustration purposes. df . head ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } playerID yearID stint teamID lgID G AB R H 2B ... SB CS BB SO IBB HBP SH SF GIDP batting_avg 101333 abadfe01 2016 1 MIN AL 39 1.0 0 0 0 ... 0.0 0.0 0 1.0 0.0 0.0 0.0 0.0 0.0 0.000000 101335 abreujo02 2016 1 CHA AL 159 624.0 67 183 32 ... 0.0 2.0 47 125.0 7.0 15.0 0.0 9.0 21.0 0.293269 101337 ackledu01 2016 1 NYA AL 28 61.0 6 9 0 ... 0.0 0.0 8 9.0 0.0 0.0 0.0 1.0 0.0 0.147541 101338 adamecr01 2016 1 COL NL 121 225.0 25 49 7 ... 2.0 3.0 24 47.0 0.0 4.0 3.0 0.0 5.0 0.217778 101340 adamsma01 2016 1 SLN NL 118 297.0 37 74 18 ... 0.0 1.0 25 81.0 1.0 2.0 0.0 3.0 5.0 0.249158 5 rows \u00d7 23 columns In this dataset, the columns AB and H are the most relevent. AB is the number of times a player was A t B at. H is the number of times a player h it the ball while batting. The performance of a player can be defined by their batting percentage - essentially the number of hits divided by the number of times at bat. (Technically, a percentage should run from 0-100, but American sportspeople are apparently not very strict with how they approach these definitions.) Model 1: Naive Model One model that we can write is a model that assumes that each player has a batting percentage that is independent of the other players in the dataset. A pictorial view of the model is as such: Let's implement this model in PyMC3. with pm . Model () as baseline_model : thetas = pm . Beta ( \"thetas\" , alpha = 0.5 , beta = 0.5 , shape = ( len ( df ))) like = pm . Binomial ( \"likelihood\" , n = df [ \"AB\" ], p = thetas , observed = df [ \"H\" ]) with baseline_model : baseline_trace = pm . sample ( 2000 ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [thetas] Sampling 4 chains, 16 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:03<00:00, 2887.97draws/s] There were 5 divergences after tuning. Increase `target_accept` or reparameterize. There were 2 divergences after tuning. Increase `target_accept` or reparameterize. There were 8 divergences after tuning. Increase `target_accept` or reparameterize. There was 1 divergence after tuning. Increase `target_accept` or reparameterize. Let's view the posterior distribution traces. import arviz as az traceplot = az . plot_trace ( baseline_trace ) Looks like convergence has been achieved. From a Beta(\\alpha=0.5, \\beta=0.5) Beta(\\alpha=0.5, \\beta=0.5) prior, those players for which we have only 1 data point have very wide posterior distribution estimates. axes = az . plot_forest ( baseline_trace ) One of the big problems that we may have observed above is that the posterior distribution estimates for some players look very \"absurd\", raising a number of questions. For example: Do we really expect the 1 bat, 1 hit player to have such a high estimated batting average? Do we really expect the 1 bat, 0 hits player to have such a low estimated batting average? Don't we usually expect human performance to be approximately symmetrically distributed around some population mean? Model 2: Hierarchical Model With a hierarchical model, we can encode a set of assumptions into the model that may help us address the above questions. By a hierarchical model, we mean that each group's key parameter (in this case, the {\\theta} {\\theta} ) have a parental distribution placed on top of them. This is sometimes anthropomorphically called \"sharing information\" between the distributions. (Not saying that this is a good or bad thing, just naming it as it is.) The model diagram looks something like this: Let's start by specifying the model. with pm . Model () as baseball_model : phi = pm . Uniform ( \"phi\" , lower = 0.0 , upper = 1.0 ) kappa_log = pm . Exponential ( \"kappa_log\" , lam = 1.5 ) kappa = pm . Deterministic ( \"kappa\" , tt . exp ( kappa_log )) thetas = pm . Beta ( \"thetas\" , alpha = phi * kappa , beta = ( 1.0 - phi ) * kappa , shape = len ( df ) ) like = pm . Binomial ( \"like\" , n = df [ \"AB\" ], p = thetas , observed = df [ \"H\" ]) Then we sample from the posterior. with baseball_model : trace = pm . sample ( 2000 ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [thetas, kappa_log, phi] Sampling 4 chains, 23 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:06<00:00, 1448.88draws/s] There were 23 divergences after tuning. Increase `target_accept` or reparameterize. The number of effective samples is smaller than 10% for some parameters. Finally, let's inspect the traces for convergence. axes = az . plot_trace ( trace ) Convergence looks good! Let's also look at the posterior distribution of batting averages per player. ylabels = \"AB: \" + df [ \"AB\" ] . astype ( str ) + \", H: \" + df [ \"H\" ] . astype ( \"str\" ) ylabels = list ( reversed ( ylabels . tolist ())) axes = az . plot_forest ( trace , var_names = [ \"thetas\" ]) axes [ 0 ] . set_yticklabels ( ylabels ); It appears to me that these estimates are going to be much more reasonable. No player has a wildly high or low estimate on the basis of a single (or very few) data points. With a hierarchical model, we make the assumption that our observations (or treatments that group our observations) are somehow related. Under this assumption, when we have a new sample for which we have very few observations, we are able to borrow power from the population to make inferences about the new sample. Depending on the scenario, this assumption can either be reasonable, thereby not necessitating much debate, or be considered a \"strong assumption\", thereby requiring strong justification. Shrinkage \"Shrinkage\" is a term used to describe how hierarchical model estimation will usually result in parameter estimates that are \"shrunk\" away from their maximum likelihood estimators (i.e. the naive estimate from the data) towards the global mean. Shrinkage in and of itself is not necessarily a good or bad thing. However, because hierarchical models can sometimes be tricky to get right, we can use a shrinkage plot as a visual diagnostic for whether we have implemented the model correctly. # MLE per player mle = df [ \"batting_avg\" ] . values # Non-hierarchical model no_pool = baseline_trace [ \"thetas\" ] . mean ( axis = 0 ) # Hierarchical model partial_pool = trace [ \"thetas\" ] . mean ( axis = 0 ) # MLE over all players complete_pool = np . array ([ df [ \"batting_avg\" ] . mean ()] * len ( df )) fig = plt . figure () ax = fig . add_subplot ( 111 ) for r in np . vstack ([ mle , no_pool , partial_pool , complete_pool ]) . T : ax . plot ( r ) ax . set_xticks ([ 0 , 1 , 2 , 3 ]) ax . set_xticklabels ( [ \"maximum \\n likelihood\" , \"no \\n pooling\" , \"partial \\n pooling\" , \"complete \\n pooling\" ] ) ax . set_ylim ( 0 , 1 ) (0.0, 1.0) In this shrinkage plot, as we go from the maximum likelihood estimator (MLE) (essentially not explicitly specifying priors) to a no-pooling model (with weak priors), there is \"shrinkage\" of the estimates towards the population mean (complete pooling). Incorporating a parental prior on each group's parameters further constrains the credible range of parameters. This, then, is the phenomenon of \"shrinkage\" in modelling. Just to reiterate again -- there is nothing ineherently right or wrong about shrinkage. Whether this is reasonable or not depends on our prior information about the problem.","title":"Hierarchical baseball"},{"location":"notebooks/hierarchical-baseball/#introduction","text":"This notebook shows how to do hierarchical modelling with Binomially-distributed random variables.","title":"Introduction"},{"location":"notebooks/hierarchical-baseball/#problem-setup","text":"Baseball players have many metrics measured for them. Let's say we are on a baseball team, and would like to quantify player performance, one metric being their batting average (defined by how many times a batter hit a pitched ball, divided by the number of times they were up for batting (\"at bat\")). How would you go about this task? We first need some measurements of batting data. To answer this question, we need to have data on the number of time a player has batted and the number of times the player has hit the ball while batting. Let's see an example dataset below. % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' import pandas as pd import pymc3 as pm import matplotlib.pyplot as plt import numpy as np import theano.tensor as tt from pyprojroot import here WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl' df = pd . read_csv ( here () / \"datasets/baseballdb/core/Batting.csv\" ) df [ \"AB\" ] = df [ \"AB\" ] . replace ( 0 , np . nan ) df = df . dropna () df [ \"batting_avg\" ] = df [ \"H\" ] / df [ \"AB\" ] df = df [ df [ \"yearID\" ] >= 2016 ] df = df . iloc [ 0 : 15 ] # select out only the first 15 players, just for illustration purposes. df . head ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } playerID yearID stint teamID lgID G AB R H 2B ... SB CS BB SO IBB HBP SH SF GIDP batting_avg 101333 abadfe01 2016 1 MIN AL 39 1.0 0 0 0 ... 0.0 0.0 0 1.0 0.0 0.0 0.0 0.0 0.0 0.000000 101335 abreujo02 2016 1 CHA AL 159 624.0 67 183 32 ... 0.0 2.0 47 125.0 7.0 15.0 0.0 9.0 21.0 0.293269 101337 ackledu01 2016 1 NYA AL 28 61.0 6 9 0 ... 0.0 0.0 8 9.0 0.0 0.0 0.0 1.0 0.0 0.147541 101338 adamecr01 2016 1 COL NL 121 225.0 25 49 7 ... 2.0 3.0 24 47.0 0.0 4.0 3.0 0.0 5.0 0.217778 101340 adamsma01 2016 1 SLN NL 118 297.0 37 74 18 ... 0.0 1.0 25 81.0 1.0 2.0 0.0 3.0 5.0 0.249158 5 rows \u00d7 23 columns In this dataset, the columns AB and H are the most relevent. AB is the number of times a player was A t B at. H is the number of times a player h it the ball while batting. The performance of a player can be defined by their batting percentage - essentially the number of hits divided by the number of times at bat. (Technically, a percentage should run from 0-100, but American sportspeople are apparently not very strict with how they approach these definitions.)","title":"Problem Setup"},{"location":"notebooks/hierarchical-baseball/#model-1-naive-model","text":"One model that we can write is a model that assumes that each player has a batting percentage that is independent of the other players in the dataset. A pictorial view of the model is as such: Let's implement this model in PyMC3. with pm . Model () as baseline_model : thetas = pm . Beta ( \"thetas\" , alpha = 0.5 , beta = 0.5 , shape = ( len ( df ))) like = pm . Binomial ( \"likelihood\" , n = df [ \"AB\" ], p = thetas , observed = df [ \"H\" ]) with baseline_model : baseline_trace = pm . sample ( 2000 ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [thetas] Sampling 4 chains, 16 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:03<00:00, 2887.97draws/s] There were 5 divergences after tuning. Increase `target_accept` or reparameterize. There were 2 divergences after tuning. Increase `target_accept` or reparameterize. There were 8 divergences after tuning. Increase `target_accept` or reparameterize. There was 1 divergence after tuning. Increase `target_accept` or reparameterize. Let's view the posterior distribution traces. import arviz as az traceplot = az . plot_trace ( baseline_trace ) Looks like convergence has been achieved. From a Beta(\\alpha=0.5, \\beta=0.5) Beta(\\alpha=0.5, \\beta=0.5) prior, those players for which we have only 1 data point have very wide posterior distribution estimates. axes = az . plot_forest ( baseline_trace ) One of the big problems that we may have observed above is that the posterior distribution estimates for some players look very \"absurd\", raising a number of questions. For example: Do we really expect the 1 bat, 1 hit player to have such a high estimated batting average? Do we really expect the 1 bat, 0 hits player to have such a low estimated batting average? Don't we usually expect human performance to be approximately symmetrically distributed around some population mean?","title":"Model 1: Naive Model"},{"location":"notebooks/hierarchical-baseball/#model-2-hierarchical-model","text":"With a hierarchical model, we can encode a set of assumptions into the model that may help us address the above questions. By a hierarchical model, we mean that each group's key parameter (in this case, the {\\theta} {\\theta} ) have a parental distribution placed on top of them. This is sometimes anthropomorphically called \"sharing information\" between the distributions. (Not saying that this is a good or bad thing, just naming it as it is.) The model diagram looks something like this: Let's start by specifying the model. with pm . Model () as baseball_model : phi = pm . Uniform ( \"phi\" , lower = 0.0 , upper = 1.0 ) kappa_log = pm . Exponential ( \"kappa_log\" , lam = 1.5 ) kappa = pm . Deterministic ( \"kappa\" , tt . exp ( kappa_log )) thetas = pm . Beta ( \"thetas\" , alpha = phi * kappa , beta = ( 1.0 - phi ) * kappa , shape = len ( df ) ) like = pm . Binomial ( \"like\" , n = df [ \"AB\" ], p = thetas , observed = df [ \"H\" ]) Then we sample from the posterior. with baseball_model : trace = pm . sample ( 2000 ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [thetas, kappa_log, phi] Sampling 4 chains, 23 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:06<00:00, 1448.88draws/s] There were 23 divergences after tuning. Increase `target_accept` or reparameterize. The number of effective samples is smaller than 10% for some parameters. Finally, let's inspect the traces for convergence. axes = az . plot_trace ( trace ) Convergence looks good! Let's also look at the posterior distribution of batting averages per player. ylabels = \"AB: \" + df [ \"AB\" ] . astype ( str ) + \", H: \" + df [ \"H\" ] . astype ( \"str\" ) ylabels = list ( reversed ( ylabels . tolist ())) axes = az . plot_forest ( trace , var_names = [ \"thetas\" ]) axes [ 0 ] . set_yticklabels ( ylabels ); It appears to me that these estimates are going to be much more reasonable. No player has a wildly high or low estimate on the basis of a single (or very few) data points. With a hierarchical model, we make the assumption that our observations (or treatments that group our observations) are somehow related. Under this assumption, when we have a new sample for which we have very few observations, we are able to borrow power from the population to make inferences about the new sample. Depending on the scenario, this assumption can either be reasonable, thereby not necessitating much debate, or be considered a \"strong assumption\", thereby requiring strong justification.","title":"Model 2: Hierarchical Model"},{"location":"notebooks/hierarchical-baseball/#shrinkage","text":"\"Shrinkage\" is a term used to describe how hierarchical model estimation will usually result in parameter estimates that are \"shrunk\" away from their maximum likelihood estimators (i.e. the naive estimate from the data) towards the global mean. Shrinkage in and of itself is not necessarily a good or bad thing. However, because hierarchical models can sometimes be tricky to get right, we can use a shrinkage plot as a visual diagnostic for whether we have implemented the model correctly. # MLE per player mle = df [ \"batting_avg\" ] . values # Non-hierarchical model no_pool = baseline_trace [ \"thetas\" ] . mean ( axis = 0 ) # Hierarchical model partial_pool = trace [ \"thetas\" ] . mean ( axis = 0 ) # MLE over all players complete_pool = np . array ([ df [ \"batting_avg\" ] . mean ()] * len ( df )) fig = plt . figure () ax = fig . add_subplot ( 111 ) for r in np . vstack ([ mle , no_pool , partial_pool , complete_pool ]) . T : ax . plot ( r ) ax . set_xticks ([ 0 , 1 , 2 , 3 ]) ax . set_xticklabels ( [ \"maximum \\n likelihood\" , \"no \\n pooling\" , \"partial \\n pooling\" , \"complete \\n pooling\" ] ) ax . set_ylim ( 0 , 1 ) (0.0, 1.0) In this shrinkage plot, as we go from the maximum likelihood estimator (MLE) (essentially not explicitly specifying priors) to a no-pooling model (with weak priors), there is \"shrinkage\" of the estimates towards the population mean (complete pooling). Incorporating a parental prior on each group's parameters further constrains the credible range of parameters. This, then, is the phenomenon of \"shrinkage\" in modelling. Just to reiterate again -- there is nothing ineherently right or wrong about shrinkage. Whether this is reasonable or not depends on our prior information about the problem.","title":"Shrinkage"}]}